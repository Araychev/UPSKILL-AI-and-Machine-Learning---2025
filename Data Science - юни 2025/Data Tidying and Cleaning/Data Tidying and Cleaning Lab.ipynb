{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078d90d4-aec6-40fe-997f-ce63c962d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa71be-5d01-4efc-85fa-a019fe92a39a",
   "metadata": {},
   "source": [
    "# Data Tidying and Cleaning Lab\n",
    "## Reading, tidying and cleaning data. Preparing data for exploration, mining, analysis and learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259927d3-4ad5-471e-b34b-114f07127a39",
   "metadata": {},
   "source": [
    "In this lab, you'll be working with the Coffee Quality Index dataset, located [here](https://www.kaggle.com/datasets/volpatto/coffee-quality-database-from-cqi). For convenience (and to save trouble in case you can't download files, or someone uploads a newer version), I've provided the dataset in the `data/` folder. The metadata (description) is at the Kaggle link. For this lab, you'll only need `merged_data_cleaned.csv`, as it is the concatenation of the other two datasets.\n",
    "\n",
    "In this (and the following labs), you'll get several questions and problems. Do your analysis, describe it, use any tools and plots you wish, and answer. You can create any amount of cells you'd like.\n",
    "\n",
    "Sometimes, the answers will not be unique, and they will depend on how you decide to approach and solve the problem. This is usual - we're doing science after all!\n",
    "\n",
    "It's a good idea to save your clean dataset after all the work you've done to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450ac8e-523c-46f8-a410-9ad5af4cfc14",
   "metadata": {},
   "source": [
    "### Problem 1. Read the dataset (1 point)\n",
    "This should be self-explanatory. The first column is the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df50bd71-ea2a-4db9-814d-c49f782ca101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded!\n",
      "Shape: (1339, 43)\n",
      "Number of rows: 1339\n",
      "Number of columns: 43\n",
      "\n",
      "Column names:\n",
      "['Species', 'Owner', 'Country.of.Origin', 'Farm.Name', 'Lot.Number', 'Mill', 'ICO.Number', 'Company', 'Altitude', 'Region', 'Producer', 'Number.of.Bags', 'Bag.Weight', 'In.Country.Partner', 'Harvest.Year', 'Grading.Date', 'Owner.1', 'Variety', 'Processing.Method', 'Aroma', 'Flavor', 'Aftertaste', 'Acidity', 'Body', 'Balance', 'Uniformity', 'Clean.Cup', 'Sweetness', 'Cupper.Points', 'Total.Cup.Points', 'Moisture', 'Category.One.Defects', 'Quakers', 'Color', 'Category.Two.Defects', 'Expiration', 'Certification.Body', 'Certification.Address', 'Certification.Contact', 'unit_of_measurement', 'altitude_low_meters', 'altitude_high_meters', 'altitude_mean_meters']\n",
      "\n",
      "First 5 rows:\n",
      "   Species                     Owner Country.of.Origin  \\\n",
      "0  Arabica                 metad plc          Ethiopia   \n",
      "1  Arabica                 metad plc          Ethiopia   \n",
      "2  Arabica  grounds for health admin         Guatemala   \n",
      "3  Arabica       yidnekachew dabessa          Ethiopia   \n",
      "4  Arabica                 metad plc          Ethiopia   \n",
      "\n",
      "                                  Farm.Name Lot.Number       Mill ICO.Number  \\\n",
      "0                                 metad plc        NaN  metad plc  2014/2015   \n",
      "1                                 metad plc        NaN  metad plc  2014/2015   \n",
      "2  san marcos barrancas \"san cristobal cuch        NaN        NaN        NaN   \n",
      "3     yidnekachew dabessa coffee plantation        NaN    wolensu        NaN   \n",
      "4                                 metad plc        NaN  metad plc  2014/2015   \n",
      "\n",
      "                                 Company       Altitude        Region  ...  \\\n",
      "0      metad agricultural developmet plc      1950-2200  guji-hambela  ...   \n",
      "1      metad agricultural developmet plc      1950-2200  guji-hambela  ...   \n",
      "2                                    NaN  1600 - 1800 m           NaN  ...   \n",
      "3  yidnekachew debessa coffee plantation      1800-2200        oromia  ...   \n",
      "4      metad agricultural developmet plc      1950-2200  guji-hambela  ...   \n",
      "\n",
      "   Color  Category.Two.Defects        Expiration  \\\n",
      "0  Green                     0   April 3rd, 2016   \n",
      "1  Green                     1   April 3rd, 2016   \n",
      "2    NaN                     0    May 31st, 2011   \n",
      "3  Green                     2  March 25th, 2016   \n",
      "4  Green                     2   April 3rd, 2016   \n",
      "\n",
      "                   Certification.Body  \\\n",
      "0  METAD Agricultural Development plc   \n",
      "1  METAD Agricultural Development plc   \n",
      "2        Specialty Coffee Association   \n",
      "3  METAD Agricultural Development plc   \n",
      "4  METAD Agricultural Development plc   \n",
      "\n",
      "                      Certification.Address  \\\n",
      "0  309fcf77415a3661ae83e027f7e5f05dad786e44   \n",
      "1  309fcf77415a3661ae83e027f7e5f05dad786e44   \n",
      "2  36d0d00a3724338ba7937c52a378d085f2172daa   \n",
      "3  309fcf77415a3661ae83e027f7e5f05dad786e44   \n",
      "4  309fcf77415a3661ae83e027f7e5f05dad786e44   \n",
      "\n",
      "                      Certification.Contact unit_of_measurement  \\\n",
      "0  19fef5a731de2db57d16da10287413f5f99bc2dd                   m   \n",
      "1  19fef5a731de2db57d16da10287413f5f99bc2dd                   m   \n",
      "2  0878a7d4b9d35ddbf0fe2ce69a2062cceb45a660                   m   \n",
      "3  19fef5a731de2db57d16da10287413f5f99bc2dd                   m   \n",
      "4  19fef5a731de2db57d16da10287413f5f99bc2dd                   m   \n",
      "\n",
      "  altitude_low_meters altitude_high_meters  altitude_mean_meters  \n",
      "0              1950.0               2200.0                2075.0  \n",
      "1              1950.0               2200.0                2075.0  \n",
      "2              1600.0               1800.0                1700.0  \n",
      "3              1800.0               2200.0                2000.0  \n",
      "4              1950.0               2200.0                2075.0  \n",
      "\n",
      "[5 rows x 43 columns]\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1339 entries, 0 to 1338\n",
      "Data columns (total 43 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Species                1339 non-null   object \n",
      " 1   Owner                  1332 non-null   object \n",
      " 2   Country.of.Origin      1338 non-null   object \n",
      " 3   Farm.Name              980 non-null    object \n",
      " 4   Lot.Number             276 non-null    object \n",
      " 5   Mill                   1021 non-null   object \n",
      " 6   ICO.Number             1180 non-null   object \n",
      " 7   Company                1130 non-null   object \n",
      " 8   Altitude               1113 non-null   object \n",
      " 9   Region                 1280 non-null   object \n",
      " 10  Producer               1107 non-null   object \n",
      " 11  Number.of.Bags         1339 non-null   int64  \n",
      " 12  Bag.Weight             1339 non-null   object \n",
      " 13  In.Country.Partner     1339 non-null   object \n",
      " 14  Harvest.Year           1292 non-null   object \n",
      " 15  Grading.Date           1339 non-null   object \n",
      " 16  Owner.1                1332 non-null   object \n",
      " 17  Variety                1113 non-null   object \n",
      " 18  Processing.Method      1169 non-null   object \n",
      " 19  Aroma                  1339 non-null   float64\n",
      " 20  Flavor                 1339 non-null   float64\n",
      " 21  Aftertaste             1339 non-null   float64\n",
      " 22  Acidity                1339 non-null   float64\n",
      " 23  Body                   1339 non-null   float64\n",
      " 24  Balance                1339 non-null   float64\n",
      " 25  Uniformity             1339 non-null   float64\n",
      " 26  Clean.Cup              1339 non-null   float64\n",
      " 27  Sweetness              1339 non-null   float64\n",
      " 28  Cupper.Points          1339 non-null   float64\n",
      " 29  Total.Cup.Points       1339 non-null   float64\n",
      " 30  Moisture               1339 non-null   float64\n",
      " 31  Category.One.Defects   1339 non-null   int64  \n",
      " 32  Quakers                1338 non-null   float64\n",
      " 33  Color                  1069 non-null   object \n",
      " 34  Category.Two.Defects   1339 non-null   int64  \n",
      " 35  Expiration             1339 non-null   object \n",
      " 36  Certification.Body     1339 non-null   object \n",
      " 37  Certification.Address  1339 non-null   object \n",
      " 38  Certification.Contact  1339 non-null   object \n",
      " 39  unit_of_measurement    1339 non-null   object \n",
      " 40  altitude_low_meters    1109 non-null   float64\n",
      " 41  altitude_high_meters   1109 non-null   float64\n",
      " 42  altitude_mean_meters   1109 non-null   float64\n",
      "dtypes: float64(16), int64(3), object(24)\n",
      "memory usage: 460.3+ KB\n",
      "None\n",
      "\n",
      "Basic statistics for numerical columns:\n",
      "       Number.of.Bags        Aroma       Flavor   Aftertaste      Acidity  \\\n",
      "count     1339.000000  1339.000000  1339.000000  1339.000000  1339.000000   \n",
      "mean       154.182972     7.566706     7.520426     7.401083     7.535706   \n",
      "std        129.987162     0.377560     0.398442     0.404463     0.379827   \n",
      "min          0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%         14.000000     7.420000     7.330000     7.250000     7.330000   \n",
      "50%        175.000000     7.580000     7.580000     7.420000     7.580000   \n",
      "75%        275.000000     7.750000     7.750000     7.580000     7.750000   \n",
      "max       1062.000000     8.750000     8.830000     8.670000     8.750000   \n",
      "\n",
      "              Body      Balance   Uniformity    Clean.Cup    Sweetness  \\\n",
      "count  1339.000000  1339.000000  1339.000000  1339.000000  1339.000000   \n",
      "mean      7.517498     7.518013     9.834877     9.835108     9.856692   \n",
      "std       0.370064     0.408943     0.554591     0.763946     0.616102   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       7.330000     7.330000    10.000000    10.000000    10.000000   \n",
      "50%       7.500000     7.500000    10.000000    10.000000    10.000000   \n",
      "75%       7.670000     7.750000    10.000000    10.000000    10.000000   \n",
      "max       8.580000     8.750000    10.000000    10.000000    10.000000   \n",
      "\n",
      "       Cupper.Points  Total.Cup.Points     Moisture  Category.One.Defects  \\\n",
      "count    1339.000000       1339.000000  1339.000000           1339.000000   \n",
      "mean        7.503376         82.089851     0.088379              0.479462   \n",
      "std         0.473464          3.500575     0.048287              2.549683   \n",
      "min         0.000000          0.000000     0.000000              0.000000   \n",
      "25%         7.250000         81.080000     0.090000              0.000000   \n",
      "50%         7.500000         82.500000     0.110000              0.000000   \n",
      "75%         7.750000         83.670000     0.120000              0.000000   \n",
      "max        10.000000         90.580000     0.280000             63.000000   \n",
      "\n",
      "           Quakers  Category.Two.Defects  altitude_low_meters  \\\n",
      "count  1338.000000           1339.000000          1109.000000   \n",
      "mean      0.173393              3.556385          1750.713315   \n",
      "std       0.832121              5.312541          8669.440545   \n",
      "min       0.000000              0.000000             1.000000   \n",
      "25%       0.000000              0.000000          1100.000000   \n",
      "50%       0.000000              2.000000          1310.640000   \n",
      "75%       0.000000              4.000000          1600.000000   \n",
      "max      11.000000             55.000000        190164.000000   \n",
      "\n",
      "       altitude_high_meters  altitude_mean_meters  \n",
      "count           1109.000000           1109.000000  \n",
      "mean            1799.347775           1775.030545  \n",
      "std             8668.805771           8668.626080  \n",
      "min                1.000000              1.000000  \n",
      "25%             1100.000000           1100.000000  \n",
      "50%             1350.000000           1310.640000  \n",
      "75%             1650.000000           1600.000000  \n",
      "max           190164.000000         190164.000000  \n"
     ]
    }
   ],
   "source": [
    "# Read the coffee dataset\n",
    "# The first column is the index as mentioned in the problem\n",
    "df = pd.read_csv('data/merged_data_cleaned.csv', index_col=0)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset successfully loaded!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "# Display column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics for numerical columns:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278e2a8-56b4-4b1a-ad71-e7b920321e37",
   "metadata": {},
   "source": [
    "### Problem 2. Observations and features (1 point)\n",
    "How many observations are there? How many features? Which features are numerical, and which are categorical?\n",
    "\n",
    "**Note:** Think about the _meaning_, not the data types. The dataset hasn't been thoroughly cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d17c46-5475-4c33-9c35-b7cf56ac41c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OBSERVATIONS AND FEATURES ANALYSIS ===\n",
      "\n",
      "Number of observations (rows): 1339\n",
      "Number of features (columns): 43\n",
      "\n",
      "==================================================\n",
      "\n",
      "COLUMN ANALYSIS (by meaning, not just data type):\n",
      "\n",
      "Column: Species\n",
      "  Data type: object\n",
      "  Unique values: 2\n",
      "  Sample values: ['Arabica', 'Arabica', 'Arabica']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Owner\n",
      "  Data type: object\n",
      "  Unique values: 315\n",
      "  Sample values: ['metad plc', 'metad plc', 'grounds for health admin']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Country.of.Origin\n",
      "  Data type: object\n",
      "  Unique values: 36\n",
      "  Sample values: ['Ethiopia', 'Ethiopia', 'Guatemala']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Farm.Name\n",
      "  Data type: object\n",
      "  Unique values: 571\n",
      "  Sample values: ['metad plc', 'metad plc', 'san marcos barrancas \"san cristobal cuch']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Lot.Number\n",
      "  Data type: object\n",
      "  Unique values: 227\n",
      "  Sample values: ['YNC-06114', '102', 'Tsoustructive 2015 Sumatra Typica']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Mill\n",
      "  Data type: object\n",
      "  Unique values: 459\n",
      "  Sample values: ['metad plc', 'metad plc', 'wolensu']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: ICO.Number\n",
      "  Data type: object\n",
      "  Unique values: 846\n",
      "  Sample values: ['2014/2015', '2014/2015', '2014/2015']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Company\n",
      "  Data type: object\n",
      "  Unique values: 281\n",
      "  Sample values: ['metad agricultural developmet plc', 'metad agricultural developmet plc', 'yidnekachew debessa coffee plantation']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Altitude\n",
      "  Data type: object\n",
      "  Unique values: 396\n",
      "  Sample values: ['1950-2200', '1950-2200', '1600 - 1800 m']\n",
      "  → CATEGORICAL (currently stored as text ranges, but conceptually numerical)\n",
      "\n",
      "Column: Region\n",
      "  Data type: object\n",
      "  Unique values: 356\n",
      "  Sample values: ['guji-hambela', 'guji-hambela', 'oromia']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Producer\n",
      "  Data type: object\n",
      "  Unique values: 692\n",
      "  Sample values: ['METAD PLC', 'METAD PLC', 'Yidnekachew Dabessa Coffee Plantation']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Number.of.Bags\n",
      "  Data type: int64\n",
      "  Unique values: 131\n",
      "  Sample values: [300, 300, 5]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Bag.Weight\n",
      "  Data type: object\n",
      "  Unique values: 56\n",
      "  Sample values: ['60 kg', '60 kg', '1']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: In.Country.Partner\n",
      "  Data type: object\n",
      "  Unique values: 27\n",
      "  Sample values: ['METAD Agricultural Development plc', 'METAD Agricultural Development plc', 'Specialty Coffee Association']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Harvest.Year\n",
      "  Data type: object\n",
      "  Unique values: 46\n",
      "  Sample values: ['2014', '2014', '2014']\n",
      "  → NUMERICAL (represents year, but may have data quality issues)\n",
      "\n",
      "Column: Grading.Date\n",
      "  Data type: object\n",
      "  Unique values: 567\n",
      "  Sample values: ['April 4th, 2015', 'April 4th, 2015', 'May 31st, 2010']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Owner.1\n",
      "  Data type: object\n",
      "  Unique values: 319\n",
      "  Sample values: ['metad plc', 'metad plc', 'Grounds for Health Admin']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Variety\n",
      "  Data type: object\n",
      "  Unique values: 29\n",
      "  Sample values: ['Other', 'Bourbon', 'Other']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Processing.Method\n",
      "  Data type: object\n",
      "  Unique values: 5\n",
      "  Sample values: ['Washed / Wet', 'Washed / Wet', 'Natural / Dry']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Aroma\n",
      "  Data type: float64\n",
      "  Unique values: 33\n",
      "  Sample values: [8.67, 8.75, 8.42]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Flavor\n",
      "  Data type: float64\n",
      "  Unique values: 35\n",
      "  Sample values: [8.83, 8.67, 8.5]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Aftertaste\n",
      "  Data type: float64\n",
      "  Unique values: 35\n",
      "  Sample values: [8.67, 8.5, 8.42]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Acidity\n",
      "  Data type: float64\n",
      "  Unique values: 31\n",
      "  Sample values: [8.75, 8.58, 8.42]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Body\n",
      "  Data type: float64\n",
      "  Unique values: 33\n",
      "  Sample values: [8.5, 8.42, 8.33]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Balance\n",
      "  Data type: float64\n",
      "  Unique values: 33\n",
      "  Sample values: [8.42, 8.42, 8.42]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Uniformity\n",
      "  Data type: float64\n",
      "  Unique values: 10\n",
      "  Sample values: [10.0, 10.0, 10.0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Clean.Cup\n",
      "  Data type: float64\n",
      "  Unique values: 11\n",
      "  Sample values: [10.0, 10.0, 10.0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Sweetness\n",
      "  Data type: float64\n",
      "  Unique values: 17\n",
      "  Sample values: [10.0, 10.0, 10.0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Cupper.Points\n",
      "  Data type: float64\n",
      "  Unique values: 42\n",
      "  Sample values: [8.75, 8.58, 9.25]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Total.Cup.Points\n",
      "  Data type: float64\n",
      "  Unique values: 180\n",
      "  Sample values: [90.58, 89.92, 89.75]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Moisture\n",
      "  Data type: float64\n",
      "  Unique values: 23\n",
      "  Sample values: [0.12, 0.12, 0.0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Category.One.Defects\n",
      "  Data type: int64\n",
      "  Unique values: 18\n",
      "  Sample values: [0, 0, 0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Quakers\n",
      "  Data type: float64\n",
      "  Unique values: 11\n",
      "  Sample values: [0.0, 0.0, 0.0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Color\n",
      "  Data type: object\n",
      "  Unique values: 3\n",
      "  Sample values: ['Green', 'Green', 'Green']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Category.Two.Defects\n",
      "  Data type: int64\n",
      "  Unique values: 38\n",
      "  Sample values: [0, 1, 0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: Expiration\n",
      "  Data type: object\n",
      "  Unique values: 566\n",
      "  Sample values: ['April 3rd, 2016', 'April 3rd, 2016', 'May 31st, 2011']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Certification.Body\n",
      "  Data type: object\n",
      "  Unique values: 26\n",
      "  Sample values: ['METAD Agricultural Development plc', 'METAD Agricultural Development plc', 'Specialty Coffee Association']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Certification.Address\n",
      "  Data type: object\n",
      "  Unique values: 32\n",
      "  Sample values: ['309fcf77415a3661ae83e027f7e5f05dad786e44', '309fcf77415a3661ae83e027f7e5f05dad786e44', '36d0d00a3724338ba7937c52a378d085f2172daa']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: Certification.Contact\n",
      "  Data type: object\n",
      "  Unique values: 29\n",
      "  Sample values: ['19fef5a731de2db57d16da10287413f5f99bc2dd', '19fef5a731de2db57d16da10287413f5f99bc2dd', '0878a7d4b9d35ddbf0fe2ce69a2062cceb45a660']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: unit_of_measurement\n",
      "  Data type: object\n",
      "  Unique values: 2\n",
      "  Sample values: ['m', 'm', 'm']\n",
      "  → CATEGORICAL (represents categories/labels)\n",
      "\n",
      "Column: altitude_low_meters\n",
      "  Data type: float64\n",
      "  Unique values: 198\n",
      "  Sample values: [1950.0, 1950.0, 1600.0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: altitude_high_meters\n",
      "  Data type: float64\n",
      "  Unique values: 198\n",
      "  Sample values: [2200.0, 2200.0, 1800.0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "Column: altitude_mean_meters\n",
      "  Data type: float64\n",
      "  Unique values: 211\n",
      "  Sample values: [2075.0, 2075.0, 1700.0]\n",
      "  → NUMERICAL (represents measurable quantities)\n",
      "\n",
      "==================================================\n",
      "\n",
      "SUMMARY:\n",
      "Total observations: 1339\n",
      "Total features: 43\n",
      "Categorical features: 23\n",
      "Numerical features: 20\n",
      "\n",
      "CATEGORICAL FEATURES (23):\n",
      " 1. Species\n",
      " 2. Owner\n",
      " 3. Country.of.Origin\n",
      " 4. Farm.Name\n",
      " 5. Lot.Number\n",
      " 6. Mill\n",
      " 7. ICO.Number\n",
      " 8. Company\n",
      " 9. Altitude\n",
      "10. Region\n",
      "11. Producer\n",
      "12. Bag.Weight\n",
      "13. In.Country.Partner\n",
      "14. Grading.Date\n",
      "15. Owner.1\n",
      "16. Variety\n",
      "17. Processing.Method\n",
      "18. Color\n",
      "19. Expiration\n",
      "20. Certification.Body\n",
      "21. Certification.Address\n",
      "22. Certification.Contact\n",
      "23. unit_of_measurement\n",
      "\n",
      "NUMERICAL FEATURES (20):\n",
      " 1. Number.of.Bags\n",
      " 2. Harvest.Year\n",
      " 3. Aroma\n",
      " 4. Flavor\n",
      " 5. Aftertaste\n",
      " 6. Acidity\n",
      " 7. Body\n",
      " 8. Balance\n",
      " 9. Uniformity\n",
      "10. Clean.Cup\n",
      "11. Sweetness\n",
      "12. Cupper.Points\n",
      "13. Total.Cup.Points\n",
      "14. Moisture\n",
      "15. Category.One.Defects\n",
      "16. Quakers\n",
      "17. Category.Two.Defects\n",
      "18. altitude_low_meters\n",
      "19. altitude_high_meters\n",
      "20. altitude_mean_meters\n",
      "\n",
      "==================================================\n",
      "IMPORTANT NOTES:\n",
      "- Classification based on MEANING, not just data types\n",
      "- Some 'object' columns like Harvest.Year contain numerical data\n",
      "- Some numerical columns might have data quality issues\n",
      "- Altitude is stored as text ranges but represents numerical concept\n",
      "- The dataset needs cleaning for proper analysis\n",
      "\n",
      "==================================================\n",
      "DATA QUALITY INSIGHTS:\n",
      "\n",
      "Harvest.Year unique values: ['08/09 crop', '1T/2011', '1t/2011', '2009 - 2010', '2009 / 2010', '2009-2010', '2009/2010', '2010', '2010-2011', '2011', '2011/2012', '2012', '2013', '2013/2014', '2014', '2014/2015', '2015', '2015/2016', '2016', '2016 / 2017', '2016/2017', '2017', '2017 / 2018', '2018', '23 July 2010', '3T/2011', '47/2010', '4T/10', '4T/2010', '4T72010', '4t/2010', '4t/2011', 'Abril - Julio', 'Abril - Julio /2011', 'August to December', 'December 2009-March 2010', 'Fall 2009', 'January 2011', 'January Through April', 'March 2010', 'May-August', 'Mayo a Julio', 'Sept 2009 - April 2010', 'Spring 2011 in Colombia.', 'TEST', 'mmm']\n",
      "\n",
      "Missing values per column:\n",
      "  Lot.Number: 1063 (79.4%)\n",
      "  Farm.Name: 359 (26.8%)\n",
      "  Mill: 318 (23.7%)\n",
      "  Color: 270 (20.2%)\n",
      "  Producer: 232 (17.3%)\n",
      "  altitude_high_meters: 230 (17.2%)\n",
      "  altitude_low_meters: 230 (17.2%)\n",
      "  altitude_mean_meters: 230 (17.2%)\n",
      "  Variety: 226 (16.9%)\n",
      "  Altitude: 226 (16.9%)\n",
      "  Company: 209 (15.6%)\n",
      "  Processing.Method: 170 (12.7%)\n",
      "  ICO.Number: 159 (11.9%)\n",
      "  Region: 59 (4.4%)\n",
      "  Harvest.Year: 47 (3.5%)\n",
      "  Owner.1: 7 (0.5%)\n",
      "  Owner: 7 (0.5%)\n",
      "  Country.of.Origin: 1 (0.1%)\n",
      "  Quakers: 1 (0.1%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Continue from Problem 1 - assuming df is already loaded\n",
    "# df = pd.read_csv('merged_data_cleaned.csv', index_col=0)\n",
    "\n",
    "print(\"=== OBSERVATIONS AND FEATURES ANALYSIS ===\\n\")\n",
    "\n",
    "# Number of observations and features\n",
    "print(f\"Number of observations (rows): {df.shape[0]}\")\n",
    "print(f\"Number of features (columns): {df.shape[1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Let's examine each column to determine if it's numerical or categorical based on MEANING\n",
    "print(\"\\nCOLUMN ANALYSIS (by meaning, not just data type):\\n\")\n",
    "\n",
    "# Define categorical and numerical features based on meaning\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "\n",
    "# Examine each column\n",
    "for col in df.columns:\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"  Data type: {df[col].dtype}\")\n",
    "    print(f\"  Unique values: {df[col].nunique()}\")\n",
    "    print(f\"  Sample values: {df[col].dropna().head(3).tolist()}\")\n",
    "    \n",
    "    # Determine if categorical or numerical based on meaning\n",
    "    if col in ['Species', 'Owner', 'Country.of.Origin', 'Farm.Name', 'Lot.Number', \n",
    "               'Mill', 'ICO.Number', 'Company', 'Region', 'Producer', 'Bag.Weight',\n",
    "               'In.Country.Partner', 'Grading.Date', 'Owner.1', 'Variety', \n",
    "               'Processing.Method', 'Color', 'Expiration', 'Certification.Body',\n",
    "               'Certification.Address', 'Certification.Contact', 'unit_of_measurement']:\n",
    "        categorical_features.append(col)\n",
    "        print(f\"  → CATEGORICAL (represents categories/labels)\")\n",
    "    \n",
    "    elif col in ['Aroma', 'Flavor', 'Aftertaste', 'Acidity', 'Body', 'Balance',\n",
    "                 'Uniformity', 'Clean.Cup', 'Sweetness', 'Cupper.Points', \n",
    "                 'Total.Cup.Points', 'Moisture', 'Category.One.Defects', \n",
    "                 'Quakers', 'Category.Two.Defects', 'altitude_low_meters',\n",
    "                 'altitude_high_meters', 'altitude_mean_meters', 'Number.of.Bags']:\n",
    "        numerical_features.append(col)\n",
    "        print(f\"  → NUMERICAL (represents measurable quantities)\")\n",
    "    \n",
    "    # Special cases that need closer inspection\n",
    "    elif col in ['Altitude', 'Harvest.Year']:\n",
    "        # Altitude is stored as text ranges but represents numerical values\n",
    "        # Harvest.Year should be numerical but might have data quality issues\n",
    "        if col == 'Altitude':\n",
    "            categorical_features.append(col)  # Currently stored as text ranges\n",
    "            print(f\"  → CATEGORICAL (currently stored as text ranges, but conceptually numerical)\")\n",
    "        else:  # Harvest.Year\n",
    "            numerical_features.append(col)\n",
    "            print(f\"  → NUMERICAL (represents year, but may have data quality issues)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(f\"Total observations: {df.shape[0]}\")\n",
    "print(f\"Total features: {df.shape[1]}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "\n",
    "print(f\"\\nCATEGORICAL FEATURES ({len(categorical_features)}):\")\n",
    "for i, feat in enumerate(categorical_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nNUMERICAL FEATURES ({len(numerical_features)}):\")\n",
    "for i, feat in enumerate(numerical_features, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"IMPORTANT NOTES:\")\n",
    "print(\"- Classification based on MEANING, not just data types\")\n",
    "print(\"- Some 'object' columns like Harvest.Year contain numerical data\")\n",
    "print(\"- Some numerical columns might have data quality issues\")\n",
    "print(\"- Altitude is stored as text ranges but represents numerical concept\")\n",
    "print(\"- The dataset needs cleaning for proper analysis\")\n",
    "\n",
    "# Let's also check for potential data quality issues\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"DATA QUALITY INSIGHTS:\")\n",
    "\n",
    "# Check Harvest.Year for non-numeric values\n",
    "print(f\"\\nHarvest.Year unique values: {sorted(df['Harvest.Year'].dropna().unique())}\")\n",
    "\n",
    "# Check missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "for col, count in missing_counts.items():\n",
    "    print(f\"  {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(missing_counts) == 0:\n",
    "    print(\"  No missing values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b745e68-35eb-4acb-b39f-513137f0ee4b",
   "metadata": {},
   "source": [
    "### Problem 3. Column manipulation (1 point)\n",
    "Make the column names more Pythonic (which helps with the quality and... aesthetics). Convert column names to `snake_case`, i.e. `species`, `country_of_origin`, `ico_number`, etc. Try to not do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "301e9f3f-afd6-4a91-a32f-1974584694ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COLUMN MANIPULATION TO SNAKE_CASE ===\n",
      "\n",
      "ORIGINAL COLUMN NAMES:\n",
      " 1. Species\n",
      " 2. Owner\n",
      " 3. Country.of.Origin\n",
      " 4. Farm.Name\n",
      " 5. Lot.Number\n",
      " 6. Mill\n",
      " 7. ICO.Number\n",
      " 8. Company\n",
      " 9. Altitude\n",
      "10. Region\n",
      "11. Producer\n",
      "12. Number.of.Bags\n",
      "13. Bag.Weight\n",
      "14. In.Country.Partner\n",
      "15. Harvest.Year\n",
      "16. Grading.Date\n",
      "17. Owner.1\n",
      "18. Variety\n",
      "19. Processing.Method\n",
      "20. Aroma\n",
      "21. Flavor\n",
      "22. Aftertaste\n",
      "23. Acidity\n",
      "24. Body\n",
      "25. Balance\n",
      "26. Uniformity\n",
      "27. Clean.Cup\n",
      "28. Sweetness\n",
      "29. Cupper.Points\n",
      "30. Total.Cup.Points\n",
      "31. Moisture\n",
      "32. Category.One.Defects\n",
      "33. Quakers\n",
      "34. Color\n",
      "35. Category.Two.Defects\n",
      "36. Expiration\n",
      "37. Certification.Body\n",
      "38. Certification.Address\n",
      "39. Certification.Contact\n",
      "40. unit_of_measurement\n",
      "41. altitude_low_meters\n",
      "42. altitude_high_meters\n",
      "43. altitude_mean_meters\n",
      "\n",
      "Total columns: 43\n",
      "\n",
      "============================================================\n",
      "COLUMN NAME TRANSFORMATION:\n",
      "============================================================\n",
      "Species                   → species\n",
      "Owner                     → owner\n",
      "Country.of.Origin         → country_of_origin\n",
      "Farm.Name                 → farm_name\n",
      "Lot.Number                → lot_number\n",
      "Mill                      → mill\n",
      "ICO.Number                → ico_number\n",
      "Company                   → company\n",
      "Altitude                  → altitude\n",
      "Region                    → region\n",
      "Producer                  → producer\n",
      "Number.of.Bags            → number_of_bags\n",
      "Bag.Weight                → bag_weight\n",
      "In.Country.Partner        → in_country_partner\n",
      "Harvest.Year              → harvest_year\n",
      "Grading.Date              → grading_date\n",
      "Owner.1                   → owner_1\n",
      "Variety                   → variety\n",
      "Processing.Method         → processing_method\n",
      "Aroma                     → aroma\n",
      "Flavor                    → flavor\n",
      "Aftertaste                → aftertaste\n",
      "Acidity                   → acidity\n",
      "Body                      → body\n",
      "Balance                   → balance\n",
      "Uniformity                → uniformity\n",
      "Clean.Cup                 → clean_cup\n",
      "Sweetness                 → sweetness\n",
      "Cupper.Points             → cupper_points\n",
      "Total.Cup.Points          → total_cup_points\n",
      "Moisture                  → moisture\n",
      "Category.One.Defects      → category_one_defects\n",
      "Quakers                   → quakers\n",
      "Color                     → color\n",
      "Category.Two.Defects      → category_two_defects\n",
      "Expiration                → expiration\n",
      "Certification.Body        → certification_body\n",
      "Certification.Address     → certification_address\n",
      "Certification.Contact     → certification_contact\n",
      "unit_of_measurement       → unit_of_measurement\n",
      "altitude_low_meters       → altitude_low_meters\n",
      "altitude_high_meters      → altitude_high_meters\n",
      "altitude_mean_meters      → altitude_mean_meters\n",
      "\n",
      "============================================================\n",
      "NEW COLUMN NAMES (snake_case):\n",
      " 1. species\n",
      " 2. owner\n",
      " 3. country_of_origin\n",
      " 4. farm_name\n",
      " 5. lot_number\n",
      " 6. mill\n",
      " 7. ico_number\n",
      " 8. company\n",
      " 9. altitude\n",
      "10. region\n",
      "11. producer\n",
      "12. number_of_bags\n",
      "13. bag_weight\n",
      "14. in_country_partner\n",
      "15. harvest_year\n",
      "16. grading_date\n",
      "17. owner_1\n",
      "18. variety\n",
      "19. processing_method\n",
      "20. aroma\n",
      "21. flavor\n",
      "22. aftertaste\n",
      "23. acidity\n",
      "24. body\n",
      "25. balance\n",
      "26. uniformity\n",
      "27. clean_cup\n",
      "28. sweetness\n",
      "29. cupper_points\n",
      "30. total_cup_points\n",
      "31. moisture\n",
      "32. category_one_defects\n",
      "33. quakers\n",
      "34. color\n",
      "35. category_two_defects\n",
      "36. expiration\n",
      "37. certification_body\n",
      "38. certification_address\n",
      "39. certification_contact\n",
      "40. unit_of_measurement\n",
      "41. altitude_low_meters\n",
      "42. altitude_high_meters\n",
      "43. altitude_mean_meters\n",
      "\n",
      "Total columns: 43\n",
      "\n",
      "============================================================\n",
      "VERIFICATION:\n",
      "Original shape: (1339, 43)\n",
      "New shape: (1339, 43)\n",
      "Data preserved: True\n",
      "\n",
      "Sample of data with new column names:\n",
      "   species                     owner country_of_origin  \\\n",
      "0  Arabica                 metad plc          Ethiopia   \n",
      "1  Arabica                 metad plc          Ethiopia   \n",
      "2  Arabica  grounds for health admin         Guatemala   \n",
      "\n",
      "                                  farm_name lot_number       mill ico_number  \\\n",
      "0                                 metad plc        NaN  metad plc  2014/2015   \n",
      "1                                 metad plc        NaN  metad plc  2014/2015   \n",
      "2  san marcos barrancas \"san cristobal cuch        NaN        NaN        NaN   \n",
      "\n",
      "                             company       altitude        region  ...  color  \\\n",
      "0  metad agricultural developmet plc      1950-2200  guji-hambela  ...  Green   \n",
      "1  metad agricultural developmet plc      1950-2200  guji-hambela  ...  Green   \n",
      "2                                NaN  1600 - 1800 m           NaN  ...    NaN   \n",
      "\n",
      "   category_two_defects       expiration                  certification_body  \\\n",
      "0                     0  April 3rd, 2016  METAD Agricultural Development plc   \n",
      "1                     1  April 3rd, 2016  METAD Agricultural Development plc   \n",
      "2                     0   May 31st, 2011        Specialty Coffee Association   \n",
      "\n",
      "                      certification_address  \\\n",
      "0  309fcf77415a3661ae83e027f7e5f05dad786e44   \n",
      "1  309fcf77415a3661ae83e027f7e5f05dad786e44   \n",
      "2  36d0d00a3724338ba7937c52a378d085f2172daa   \n",
      "\n",
      "                      certification_contact unit_of_measurement  \\\n",
      "0  19fef5a731de2db57d16da10287413f5f99bc2dd                   m   \n",
      "1  19fef5a731de2db57d16da10287413f5f99bc2dd                   m   \n",
      "2  0878a7d4b9d35ddbf0fe2ce69a2062cceb45a660                   m   \n",
      "\n",
      "  altitude_low_meters altitude_high_meters  altitude_mean_meters  \n",
      "0              1950.0               2200.0                2075.0  \n",
      "1              1950.0               2200.0                2075.0  \n",
      "2              1600.0               1800.0                1700.0  \n",
      "\n",
      "[3 rows x 43 columns]\n",
      "\n",
      "DataFrame successfully updated with snake_case column names!\n",
      "\n",
      "============================================================\n",
      "EXAMPLES OF TRANSFORMATIONS:\n",
      "✓ Country.of.Origin         → country_of_origin\n",
      "✓ ICO.Number                → ico_number\n",
      "✓ Total.Cup.Points          → total_cup_points\n",
      "✓ Category.One.Defects      → category_one_defects\n",
      "✓ Processing.Method         → processing_method\n"
     ]
    }
   ],
   "source": [
    "# Continue from previous problems - assuming df is already loaded\n",
    "# df = pd.read_csv('merged_data_cleaned.csv', index_col=0)\n",
    "\n",
    "print(\"=== COLUMN MANIPULATION TO SNAKE_CASE ===\\n\")\n",
    "\n",
    "# Display original column names\n",
    "print(\"ORIGINAL COLUMN NAMES:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nTotal columns: {len(df.columns)}\")\n",
    "\n",
    "# Function to convert to snake_case\n",
    "def to_snake_case(name):\n",
    "    \"\"\"\n",
    "    Convert column name to snake_case format\n",
    "    \"\"\"\n",
    "    # Handle special cases and clean up\n",
    "    name = str(name).strip()\n",
    "    \n",
    "    # Replace dots with underscores\n",
    "    name = name.replace('.', '_')\n",
    "    \n",
    "    # Replace spaces with underscores\n",
    "    name = name.replace(' ', '_')\n",
    "    \n",
    "    # Replace hyphens with underscores\n",
    "    name = name.replace('-', '_')\n",
    "    \n",
    "    # Insert underscore before uppercase letters (for camelCase)\n",
    "    name = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', name)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    name = name.lower()\n",
    "    \n",
    "    # Remove multiple consecutive underscores\n",
    "    name = re.sub(r'_+', '_', name)\n",
    "    \n",
    "    # Remove leading/trailing underscores\n",
    "    name = name.strip('_')\n",
    "    \n",
    "    return name\n",
    "\n",
    "# Create mapping of old to new column names\n",
    "column_mapping = {}\n",
    "for col in df.columns:\n",
    "    new_name = to_snake_case(col)\n",
    "    column_mapping[col] = new_name\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN NAME TRANSFORMATION:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for old_name, new_name in column_mapping.items():\n",
    "    print(f\"{old_name:<25} → {new_name}\")\n",
    "\n",
    "# Apply the transformation\n",
    "df_renamed = df.rename(columns=column_mapping)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"NEW COLUMN NAMES (snake_case):\")\n",
    "for i, col in enumerate(df_renamed.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nTotal columns: {len(df_renamed.columns)}\")\n",
    "\n",
    "# Verify the transformation worked\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION:\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"New shape: {df_renamed.shape}\")\n",
    "print(f\"Data preserved: {df.equals(df_renamed.rename(columns={v: k for k, v in column_mapping.items()}))}\")\n",
    "\n",
    "# Show sample of transformed data\n",
    "print(f\"\\nSample of data with new column names:\")\n",
    "print(df_renamed.head(3))\n",
    "\n",
    "# Update the main dataframe\n",
    "df = df_renamed\n",
    "print(f\"\\nDataFrame successfully updated with snake_case column names!\")\n",
    "\n",
    "# Display some examples of the transformation\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLES OF TRANSFORMATIONS:\")\n",
    "examples = [\n",
    "    ('Country.of.Origin', 'country_of_origin'),\n",
    "    ('ICO.Number', 'ico_number'), \n",
    "    ('Total.Cup.Points', 'total_cup_points'),\n",
    "    ('Category.One.Defects', 'category_one_defects'),\n",
    "    ('Processing.Method', 'processing_method')\n",
    "]\n",
    "\n",
    "for old, expected in examples:\n",
    "    actual = column_mapping.get(old, 'NOT FOUND')\n",
    "    status = \"✓\" if actual == expected else \"✗\"\n",
    "    print(f\"{status} {old:<25} → {actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c53c4c6-6eb9-4c92-bd39-89286fe4c86e",
   "metadata": {},
   "source": [
    "### Problem 4. Bag weight (1 point)\n",
    "What's up with the bag weights? Make all necessary changes to the column values. Don't forget to document your methods and assumptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e97f3d0-1223-49a7-99a9-51a65e4ff4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BAG WEIGHT ANALYSIS AND CLEANING ===\n",
      "\n",
      "ORIGINAL BAG WEIGHT ANALYSIS:\n",
      "Total records: 1339\n",
      "Non-null bag weights: 1339\n",
      "Null bag weights: 0\n",
      "\n",
      "Unique bag weight values (top 20):\n",
      "bag_weight\n",
      "1 kg       331\n",
      "60 kg      256\n",
      "69 kg      200\n",
      "70 kg      156\n",
      "2 kg       122\n",
      "100 lbs     59\n",
      "30 kg       29\n",
      "5 lbs       23\n",
      "6           19\n",
      "20 kg       14\n",
      "50 kg       14\n",
      "10 kg       11\n",
      "59 kg       10\n",
      "1 lbs        8\n",
      "1            7\n",
      "3 lbs        7\n",
      "5 kg         7\n",
      "2 lbs        5\n",
      "4 lbs        4\n",
      "80 kg        4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique values: 56\n",
      "\n",
      "============================================================\n",
      "PATTERN ANALYSIS:\n",
      "Values with 'kg': 1198\n",
      "Values with 'lb'/'lbs': 116\n",
      "Pure numeric (no unit): 27\n",
      "\n",
      "Problematic/unusual values:\n",
      "bag_weight\n",
      "660 kg      1\n",
      "1218 kg     1\n",
      "2           1\n",
      "18 kg       1\n",
      "150 lbs     1\n",
      "18000 kg    1\n",
      "1 kg,lbs    1\n",
      "132 lbs     1\n",
      "34 kg       1\n",
      "130 lbs     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "CLEANING PROCESS:\n",
      "Applying cleaning function...\n",
      "\n",
      "CLEANING RESULTS:\n",
      "Original non-null values: 1339\n",
      "Cleaned non-null values: 1321\n",
      "Values removed as invalid: 18\n",
      "\n",
      "BEFORE/AFTER EXAMPLES:\n",
      "'60 kg' → 60.0 kg\n",
      "'60 kg' → 60.0 kg\n",
      "'1' → 1.0 kg\n",
      "'60 kg' → 60.0 kg\n",
      "'60 kg' → 60.0 kg\n",
      "'30 kg' → 30.0 kg\n",
      "'69 kg' → 69.0 kg\n",
      "'60 kg' → 60.0 kg\n",
      "'60 kg' → 60.0 kg\n",
      "'60 kg' → 60.0 kg\n",
      "'60 kg' → 60.0 kg\n",
      "'1 kg' → 1.0 kg\n",
      "'1 kg' → 1.0 kg\n",
      "'1' → 1.0 kg\n",
      "'60 kg' → 60.0 kg\n",
      "\n",
      "============================================================\n",
      "CLEANED WEIGHT DISTRIBUTION:\n",
      "count    1321.000000\n",
      "mean       36.733899\n",
      "std        36.093810\n",
      "min         0.450000\n",
      "25%         1.000000\n",
      "50%        45.360000\n",
      "75%        69.000000\n",
      "max       660.000000\n",
      "Name: bag_weight_cleaned, dtype: float64\n",
      "\n",
      "Most common bag weights (kg):\n",
      "bag_weight_cleaned\n",
      "1.00     339\n",
      "60.00    256\n",
      "69.00    200\n",
      "70.00    156\n",
      "2.00     124\n",
      "45.36     59\n",
      "30.00     29\n",
      "2.27      23\n",
      "6.00      21\n",
      "50.00     14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Potential outliers (> 200 kg):\n",
      "bag_weight_cleaned\n",
      "660.0    1\n",
      "350.0    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "FINAL STEP:\n",
      "Replacing original bag_weight column with cleaned version...\n",
      "✓ Bag weight column successfully cleaned and standardized to kg\n",
      "\n",
      "============================================================\n",
      "DOCUMENTATION OF METHODS AND ASSUMPTIONS:\n",
      "\n",
      "1. STANDARDIZATION: All weights converted to kilograms (kg)\n",
      "\n",
      "2. UNIT CONVERSIONS:\n",
      "   - Pounds to kg: 1 lb = 0.453592 kg\n",
      "   - Values with 'kg' kept as-is\n",
      "   - Pure numeric values assumed to be kg (most common pattern)\n",
      "\n",
      "3. DATA CLEANING:\n",
      "   - Removed weights <= 0 kg (invalid)\n",
      "   - Removed weights > 1000 kg (likely total shipment, not individual bag)\n",
      "   - Handled mixed units like \"2 kg,lbs\" by taking first valid number\n",
      "\n",
      "4. ASSUMPTIONS:\n",
      "   - Coffee bags typically range 1-100 kg (industry standard)\n",
      "   - Pure numeric values without units are kg (based on data pattern analysis)\n",
      "   - Values > 1000 kg are data entry errors (total weight vs bag weight)\n",
      "\n",
      "5. VALIDATION:\n",
      "   - Rounded to 2 decimal places for consistency\n",
      "   - Converted invalid entries to NaN for proper handling\n",
      "\n",
      "Final bag_weight column statistics:\n",
      "count    1321.000000\n",
      "mean       36.733899\n",
      "std        36.093810\n",
      "min         0.450000\n",
      "25%         1.000000\n",
      "50%        45.360000\n",
      "75%        69.000000\n",
      "max       660.000000\n",
      "Name: bag_weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Continue from previous problems - assuming df is already loaded with snake_case columns\n",
    "# df = pd.read_csv('merged_data_cleaned.csv', index_col=0)\n",
    "# df.columns = [to_snake_case(col) for col in df.columns]  # from Problem 3\n",
    "\n",
    "print(\"=== BAG WEIGHT ANALYSIS AND CLEANING ===\\n\")\n",
    "\n",
    "# First, let's examine what's in the bag_weight column\n",
    "print(\"ORIGINAL BAG WEIGHT ANALYSIS:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Non-null bag weights: {df['bag_weight'].notna().sum()}\")\n",
    "print(f\"Null bag weights: {df['bag_weight'].isna().sum()}\")\n",
    "\n",
    "# Show unique values and their counts\n",
    "print(f\"\\nUnique bag weight values (top 20):\")\n",
    "bag_weight_counts = df['bag_weight'].value_counts()\n",
    "print(bag_weight_counts.head(20))\n",
    "\n",
    "print(f\"\\nTotal unique values: {len(bag_weight_counts)}\")\n",
    "\n",
    "# Analyze patterns\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"PATTERN ANALYSIS:\")\n",
    "\n",
    "# Check for different units and formats\n",
    "kg_pattern = df['bag_weight'].astype(str).str.contains('kg', na=False)\n",
    "lb_pattern = df['bag_weight'].astype(str).str.contains('lb', na=False)\n",
    "numeric_only = df['bag_weight'].astype(str).str.match(r'^\\d+\\.?\\d*$', na=False)\n",
    "\n",
    "print(f\"Values with 'kg': {kg_pattern.sum()}\")\n",
    "print(f\"Values with 'lb'/'lbs': {lb_pattern.sum()}\")  \n",
    "print(f\"Pure numeric (no unit): {numeric_only.sum()}\")\n",
    "\n",
    "# Show some problematic cases\n",
    "print(f\"\\nProblematic/unusual values:\")\n",
    "unusual_values = df['bag_weight'].value_counts().tail(10)\n",
    "print(unusual_values)\n",
    "\n",
    "# Define cleaning function\n",
    "def clean_bag_weight(weight_str):\n",
    "    \"\"\"\n",
    "    Clean and standardize bag weight values.\n",
    "    \n",
    "    Assumptions and methods:\n",
    "    1. Convert all weights to kilograms (kg) as the standard unit\n",
    "    2. 1 lb = 0.453592 kg (standard conversion)\n",
    "    3. Pure numeric values without units are assumed to be kg (most common pattern)\n",
    "    4. Remove obvious data entry errors (unrealistic weights)\n",
    "    5. Handle mixed units like \"2 kg,lbs\" by taking the first valid number and unit\n",
    "    \n",
    "    Returns: float value in kg, or NaN for invalid entries\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(weight_str):\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    weight_str = str(weight_str).strip().lower()\n",
    "    \n",
    "    # Handle empty strings\n",
    "    if not weight_str or weight_str == 'nan':\n",
    "        return np.nan\n",
    "    \n",
    "    # Extract numeric value and unit using regex\n",
    "    # Look for patterns like: \"60 kg\", \"100 lbs\", \"2\", \"2 kg,lbs\"\n",
    "    match = re.search(r'(\\d+\\.?\\d*)\\s*(kg|lb|lbs)?', weight_str)\n",
    "    \n",
    "    if not match:\n",
    "        return np.nan\n",
    "    \n",
    "    numeric_value = float(match.group(1))\n",
    "    unit = match.group(2) if match.group(2) else 'kg'  # Assume kg if no unit\n",
    "    \n",
    "    # Convert to kg\n",
    "    if unit in ['lb', 'lbs']:\n",
    "        weight_kg = numeric_value * 0.453592  # Convert pounds to kg\n",
    "    else:  # kg or no unit (assumed kg)\n",
    "        weight_kg = numeric_value\n",
    "    \n",
    "    # Data validation: remove unrealistic weights\n",
    "    # Coffee bags typically range from 1 kg to 100 kg\n",
    "    # Anything above 1000 kg is likely an error (total shipment weight, not individual bag)\n",
    "    if weight_kg <= 0 or weight_kg > 1000:\n",
    "        return np.nan\n",
    "    \n",
    "    return round(weight_kg, 2)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANING PROCESS:\")\n",
    "\n",
    "# Apply cleaning function\n",
    "print(\"Applying cleaning function...\")\n",
    "df['bag_weight_cleaned'] = df['bag_weight'].apply(clean_bag_weight)\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nCLEANING RESULTS:\")\n",
    "print(f\"Original non-null values: {df['bag_weight'].notna().sum()}\")\n",
    "print(f\"Cleaned non-null values: {df['bag_weight_cleaned'].notna().sum()}\")\n",
    "print(f\"Values removed as invalid: {df['bag_weight'].notna().sum() - df['bag_weight_cleaned'].notna().sum()}\")\n",
    "\n",
    "# Show before/after comparison for some examples\n",
    "print(f\"\\nBEFORE/AFTER EXAMPLES:\")\n",
    "comparison_sample = df[['bag_weight', 'bag_weight_cleaned']].dropna(subset=['bag_weight']).head(15)\n",
    "for idx, row in comparison_sample.iterrows():\n",
    "    original = row['bag_weight']\n",
    "    cleaned = row['bag_weight_cleaned']\n",
    "    print(f\"'{original}' → {cleaned} kg\")\n",
    "\n",
    "# Show distribution of cleaned weights\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"CLEANED WEIGHT DISTRIBUTION:\")\n",
    "print(df['bag_weight_cleaned'].describe())\n",
    "\n",
    "print(f\"\\nMost common bag weights (kg):\")\n",
    "print(df['bag_weight_cleaned'].value_counts().head(10))\n",
    "\n",
    "# Check for outliers that might need attention\n",
    "print(f\"\\nPotential outliers (> 200 kg):\")\n",
    "outliers = df[df['bag_weight_cleaned'] > 200]['bag_weight_cleaned'].value_counts()\n",
    "if len(outliers) > 0:\n",
    "    print(outliers)\n",
    "else:\n",
    "    print(\"No outliers found\")\n",
    "\n",
    "# Final step: replace original column\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL STEP:\")\n",
    "print(\"Replacing original bag_weight column with cleaned version...\")\n",
    "\n",
    "df['bag_weight'] = df['bag_weight_cleaned']\n",
    "df.drop('bag_weight_cleaned', axis=1, inplace=True)\n",
    "\n",
    "print(\"✓ Bag weight column successfully cleaned and standardized to kg\")\n",
    "\n",
    "# Summary of changes made\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DOCUMENTATION OF METHODS AND ASSUMPTIONS:\")\n",
    "print(\"\"\"\n",
    "1. STANDARDIZATION: All weights converted to kilograms (kg)\n",
    "\n",
    "2. UNIT CONVERSIONS:\n",
    "   - Pounds to kg: 1 lb = 0.453592 kg\n",
    "   - Values with 'kg' kept as-is\n",
    "   - Pure numeric values assumed to be kg (most common pattern)\n",
    "\n",
    "3. DATA CLEANING:\n",
    "   - Removed weights <= 0 kg (invalid)\n",
    "   - Removed weights > 1000 kg (likely total shipment, not individual bag)\n",
    "   - Handled mixed units like \"2 kg,lbs\" by taking first valid number\n",
    "\n",
    "4. ASSUMPTIONS:\n",
    "   - Coffee bags typically range 1-100 kg (industry standard)\n",
    "   - Pure numeric values without units are kg (based on data pattern analysis)\n",
    "   - Values > 1000 kg are data entry errors (total weight vs bag weight)\n",
    "\n",
    "5. VALIDATION:\n",
    "   - Rounded to 2 decimal places for consistency\n",
    "   - Converted invalid entries to NaN for proper handling\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Final bag_weight column statistics:\")\n",
    "print(df['bag_weight'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff1f817-8e5e-4185-a320-dde7ed2cada1",
   "metadata": {},
   "source": [
    "### Problem 5. Dates (1 point)\n",
    "This should remind you of problem 4 but it's slightly nastier. Fix the harvest years, document the process.\n",
    "\n",
    "While you're here, fix the expiration dates, and grading dates. Unlike the other column, these should be dates (`pd.to_datetime()` is your friend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "814dc5f9-9572-44f0-bff6-8efeb084f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATE COLUMNS CLEANING ===\n",
      "\n",
      "HARVEST_YEAR ANALYSIS:\n",
      "Total records: 1339\n",
      "Non-null values: 1292\n",
      "Null values: 47\n",
      "Unique values: 46\n",
      "Sample values:\n",
      "harvest_year\n",
      "2012           354\n",
      "2014           233\n",
      "2013           181\n",
      "2015           129\n",
      "2016           124\n",
      "2017            70\n",
      "2013/2014       29\n",
      "2015/2016       28\n",
      "2011            26\n",
      "2017 / 2018     19\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "GRADING_DATE ANALYSIS:\n",
      "Total records: 1339\n",
      "Non-null values: 1339\n",
      "Null values: 0\n",
      "Unique values: 567\n",
      "Sample values:\n",
      "grading_date\n",
      "July 11th, 2012         25\n",
      "December 26th, 2013     24\n",
      "June 6th, 2012          19\n",
      "August 30th, 2012       18\n",
      "July 26th, 2012         15\n",
      "October 8th, 2015       13\n",
      "March 29th, 2013        13\n",
      "September 27th, 2012    13\n",
      "June 17th, 2010         12\n",
      "October 20th, 2017      11\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "EXPIRATION ANALYSIS:\n",
      "Total records: 1339\n",
      "Non-null values: 1339\n",
      "Null values: 0\n",
      "Unique values: 566\n",
      "Sample values:\n",
      "expiration\n",
      "July 11th, 2013         25\n",
      "December 26th, 2014     25\n",
      "June 6th, 2013          19\n",
      "August 30th, 2013       18\n",
      "July 26th, 2013         15\n",
      "October 7th, 2016       13\n",
      "March 29th, 2014        13\n",
      "September 27th, 2013    13\n",
      "June 17th, 2011         12\n",
      "October 20th, 2018      11\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "1. CLEANING HARVEST YEAR\n",
      "============================================================\n",
      "Cleaning harvest_year column...\n",
      "HARVEST YEAR CLEANING RESULTS:\n",
      "Original non-null values: 1292\n",
      "Cleaned non-null values: 1279\n",
      "Values converted to NaN: 13\n",
      "\n",
      "BEFORE/AFTER EXAMPLES:\n",
      "'March 2010' → 2010.0\n",
      "'March 2010' → 2010.0\n",
      "'Sept 2009 - April 2010' → 2009.0\n",
      "'March 2010' → 2010.0\n",
      "'May-August' → nan\n",
      "'2009/2010' → 2009.0\n",
      "'2009/2010' → 2009.0\n",
      "'2015/2016' → 2015.0\n",
      "'2009/2010' → 2009.0\n",
      "'2009/2010' → 2009.0\n",
      "\n",
      "Cleaned harvest year distribution:\n",
      "harvest_year\n",
      "2009.0     20\n",
      "2010.0     29\n",
      "2011.0     36\n",
      "2012.0    354\n",
      "2013.0    210\n",
      "2014.0    252\n",
      "2015.0    157\n",
      "2016.0    131\n",
      "2017.0     89\n",
      "2018.0      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "2. CLEANING GRADING DATE\n",
      "============================================================\n",
      "Cleaning grading_date column...\n",
      "GRADING DATE CLEANING RESULTS:\n",
      "Original non-null values: 1339\n",
      "Cleaned non-null values: 1339\n",
      "Successfully converted: 1339\n",
      "Failed conversions: 0\n",
      "\n",
      "GRADING DATE EXAMPLES:\n",
      "'April 4th, 2015' → 2015-04-04\n",
      "'April 4th, 2015' → 2015-04-04\n",
      "'May 31st, 2010' → 2010-05-31\n",
      "'March 26th, 2015' → 2015-03-26\n",
      "'April 4th, 2015' → 2015-04-04\n",
      "'September 3rd, 2013' → 2013-09-03\n",
      "'September 17th, 2012' → 2012-09-17\n",
      "'September 2nd, 2010' → 2010-09-02\n",
      "'September 2nd, 2010' → 2010-09-02\n",
      "'March 30th, 2015' → 2015-03-30\n",
      "\n",
      "============================================================\n",
      "3. CLEANING EXPIRATION DATE\n",
      "============================================================\n",
      "Cleaning expiration column...\n",
      "EXPIRATION DATE CLEANING RESULTS:\n",
      "Original non-null values: 1339\n",
      "Cleaned non-null values: 1339\n",
      "Successfully converted: 1339\n",
      "Failed conversions: 0\n",
      "\n",
      "EXPIRATION DATE EXAMPLES:\n",
      "'April 3rd, 2016' → 2016-04-03\n",
      "'April 3rd, 2016' → 2016-04-03\n",
      "'May 31st, 2011' → 2011-05-31\n",
      "'March 25th, 2016' → 2016-03-25\n",
      "'April 3rd, 2016' → 2016-04-03\n",
      "'September 3rd, 2014' → 2014-09-03\n",
      "'September 17th, 2013' → 2013-09-17\n",
      "'September 2nd, 2011' → 2011-09-02\n",
      "'September 2nd, 2011' → 2011-09-02\n",
      "'March 29th, 2016' → 2016-03-29\n",
      "\n",
      "============================================================\n",
      "4. DATA VALIDATION AND SUMMARY\n",
      "============================================================\n",
      "DATE RELATIONSHIP VALIDATION:\n",
      "Records with both grading and expiration dates: 1339\n",
      "Records where grading < expiration: 1339\n",
      "Records with date order issues: 0\n",
      "\n",
      "HARVEST vs GRADING YEAR:\n",
      "Records with both harvest year and grading date: 1279\n",
      "Average time from harvest to grading: 0.3 years\n",
      "Year difference distribution:\n",
      "-1.0      2\n",
      " 0.0    924\n",
      " 1.0    344\n",
      " 2.0      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY AND DOCUMENTATION\n",
      "============================================================\n",
      "\n",
      "DATE CLEANING DOCUMENTATION:\n",
      "\n",
      "1. HARVEST YEAR CLEANING:\n",
      "   - Converted to integer years (int64)\n",
      "   - Handled range formats (2013/2014 → 2013)\n",
      "   - Handled seasonal formats (2017 / 2018 → 2017) \n",
      "   - Handled quarter formats (4T/10 → 2010)\n",
      "   - Removed invalid text entries (Mayo a Julio → NaN)\n",
      "   - Valid range: 1900-2025\n",
      "\n",
      "2. GRADING DATE CLEANING:\n",
      "   - Converted to pandas datetime objects\n",
      "   - Handled ordinal suffixes (4th, 1st, 2nd, 3rd)\n",
      "   - Primary format: \"Month DDth, YYYY\" \n",
      "   - Fallback to multiple date format attempts\n",
      "   - Invalid dates converted to NaT\n",
      "\n",
      "3. EXPIRATION DATE CLEANING:\n",
      "   - Same approach as grading dates\n",
      "   - Converted to pandas datetime objects\n",
      "   - Consistent format handling\n",
      "\n",
      "4. DATA TYPES AFTER CLEANING:\n",
      "   - harvest_year: int64 (or float64 with NaN)\n",
      "   - grading_date: datetime64[ns]\n",
      "   - expiration: datetime64[ns]\n",
      "\n",
      "5. VALIDATION:\n",
      "   - Checked logical date relationships\n",
      "   - Verified harvest-to-grading time patterns\n",
      "   - Maintained data integrity\n",
      "\n",
      "Final data types:\n",
      "harvest_year: float64\n",
      "grading_date: datetime64[ns]\n",
      "expiration: datetime64[ns]\n",
      "\n",
      "Final non-null counts:\n",
      "harvest_year: 1279/1339 (95.5%)\n",
      "grading_date: 1339/1339 (100.0%)\n",
      "expiration: 1339/1339 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Continue from previous problems - assuming df is already loaded with snake_case columns\n",
    "# df = pd.read_csv('merged_data_cleaned.csv', index_col=0)\n",
    "# df.columns = [to_snake_case(col) for col in df.columns]  # from Problem 3\n",
    "\n",
    "print(\"=== DATE COLUMNS CLEANING ===\\n\")\n",
    "\n",
    "# First, let's examine all three date columns\n",
    "date_columns = ['harvest_year', 'grading_date', 'expiration']\n",
    "\n",
    "for col in date_columns:\n",
    "    print(f\"{col.upper()} ANALYSIS:\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Non-null values: {df[col].notna().sum()}\")\n",
    "    print(f\"Null values: {df[col].isna().sum()}\")\n",
    "    print(f\"Unique values: {df[col].nunique()}\")\n",
    "    print(f\"Sample values:\")\n",
    "    print(df[col].value_counts().head(10))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. CLEANING HARVEST YEAR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def clean_harvest_year(year_str):\n",
    "    \"\"\"\n",
    "    Clean and standardize harvest year values.\n",
    "    \n",
    "    Methods and assumptions:\n",
    "    1. Extract valid 4-digit years (1900-2025)\n",
    "    2. Handle range formats like \"2013/2014\" by taking the first year\n",
    "    3. Handle seasonal formats like \"2017 / 2018\" \n",
    "    4. Remove obviously invalid entries (non-year formats)\n",
    "    5. Convert Spanish months and other text to NaN\n",
    "    \n",
    "    Returns: int year or NaN for invalid entries\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(year_str):\n",
    "        return np.nan\n",
    "    \n",
    "    year_str = str(year_str).strip()\n",
    "    \n",
    "    # Handle empty strings\n",
    "    if not year_str or year_str.lower() == 'nan':\n",
    "        return np.nan\n",
    "    \n",
    "    # Try to extract 4-digit year from various formats\n",
    "    # Pattern 1: Simple 4-digit year (2012, 2013, etc.)\n",
    "    simple_year_match = re.search(r'\\b(19|20)\\d{2}\\b', year_str)\n",
    "    if simple_year_match:\n",
    "        year = int(simple_year_match.group())\n",
    "        if 1900 <= year <= 2025:  # Reasonable range for coffee harvesting\n",
    "            return year\n",
    "    \n",
    "    # Pattern 2: Range formats like \"2013/2014\", \"2017 / 2018\", \"2010-2011\"\n",
    "    range_match = re.search(r'\\b(19|20)\\d{2}\\s*[/\\-]\\s*(19|20)?\\d{2}\\b', year_str)\n",
    "    if range_match:\n",
    "        # Take the first year from the range\n",
    "        first_year_match = re.search(r'\\b(19|20)\\d{2}\\b', year_str)\n",
    "        if first_year_match:\n",
    "            year = int(first_year_match.group())\n",
    "            if 1900 <= year <= 2025:\n",
    "                return year\n",
    "    \n",
    "    # Pattern 3: Quarter formats like \"4T/10\", \"4T/2010\"\n",
    "    quarter_match = re.search(r'4T/(\\d{2,4})', year_str)\n",
    "    if quarter_match:\n",
    "        year_part = quarter_match.group(1)\n",
    "        if len(year_part) == 2:\n",
    "            # Assume 20XX for 2-digit years\n",
    "            year = 2000 + int(year_part)\n",
    "        else:\n",
    "            year = int(year_part)\n",
    "        if 1900 <= year <= 2025:\n",
    "            return year\n",
    "    \n",
    "    # Pattern 4: Month year formats like \"March 2010\"\n",
    "    month_year_match = re.search(r'\\b(19|20)\\d{2}\\b', year_str)\n",
    "    if month_year_match:\n",
    "        year = int(month_year_match.group())\n",
    "        if 1900 <= year <= 2025:\n",
    "            return year\n",
    "    \n",
    "    # If no valid pattern found, return NaN\n",
    "    return np.nan\n",
    "\n",
    "# Apply harvest year cleaning\n",
    "print(\"Cleaning harvest_year column...\")\n",
    "df['harvest_year_original'] = df['harvest_year'].copy()  # Keep original for comparison\n",
    "df['harvest_year'] = df['harvest_year'].apply(clean_harvest_year)\n",
    "\n",
    "print(f\"HARVEST YEAR CLEANING RESULTS:\")\n",
    "print(f\"Original non-null values: {df['harvest_year_original'].notna().sum()}\")\n",
    "print(f\"Cleaned non-null values: {df['harvest_year'].notna().sum()}\")\n",
    "print(f\"Values converted to NaN: {df['harvest_year_original'].notna().sum() - df['harvest_year'].notna().sum()}\")\n",
    "\n",
    "# Show before/after examples\n",
    "print(f\"\\nBEFORE/AFTER EXAMPLES:\")\n",
    "comparison = df[['harvest_year_original', 'harvest_year']].dropna(subset=['harvest_year_original'])\n",
    "problematic_examples = comparison[comparison['harvest_year_original'].astype(str).str.contains(r'[/\\-]|T/|[a-zA-Z]', na=False)].head(10)\n",
    "for idx, row in problematic_examples.iterrows():\n",
    "    print(f\"'{row['harvest_year_original']}' → {row['harvest_year']}\")\n",
    "\n",
    "print(f\"\\nCleaned harvest year distribution:\")\n",
    "print(df['harvest_year'].value_counts().sort_index().head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. CLEANING GRADING DATE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def clean_date_column(date_str, column_name):\n",
    "    \"\"\"\n",
    "    Clean and convert date strings to pandas datetime objects.\n",
    "    \n",
    "    Methods:\n",
    "    1. Handle format: \"Month DDth, YYYY\" (e.g., \"April 4th, 2015\")\n",
    "    2. Handle format: \"Month DDst/nd/rd/th, YYYY\"\n",
    "    3. Use pd.to_datetime with multiple format attempts\n",
    "    4. Return NaT for invalid dates\n",
    "    \n",
    "    Returns: pandas datetime or NaT\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "    \n",
    "    date_str = str(date_str).strip()\n",
    "    \n",
    "    if not date_str or date_str.lower() == 'nan':\n",
    "        return pd.NaT\n",
    "    \n",
    "    try:\n",
    "        # Try multiple approaches to parse the date\n",
    "        \n",
    "        # Method 1: Direct pd.to_datetime (handles many formats automatically)\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, errors='raise')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 2: Handle ordinal suffixes (1st, 2nd, 3rd, 4th, etc.)\n",
    "        # Remove ordinal suffixes: st, nd, rd, th\n",
    "        cleaned_date = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_str)\n",
    "        try:\n",
    "            return pd.to_datetime(cleaned_date, errors='raise')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Method 3: Try common date formats explicitly\n",
    "        date_formats = [\n",
    "            '%B %d, %Y',      # April 4, 2015\n",
    "            '%b %d, %Y',      # Apr 4, 2015\n",
    "            '%m/%d/%Y',       # 04/04/2015\n",
    "            '%Y-%m-%d',       # 2015-04-04\n",
    "            '%d/%m/%Y',       # 04/04/2015 (day/month/year)\n",
    "        ]\n",
    "        \n",
    "        for fmt in date_formats:\n",
    "            try:\n",
    "                return pd.to_datetime(cleaned_date, format=fmt, errors='raise')\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # If all methods fail, return NaT\n",
    "        return pd.NaT\n",
    "        \n",
    "    except Exception as e:\n",
    "        return pd.NaT\n",
    "\n",
    "# Clean grading date\n",
    "print(\"Cleaning grading_date column...\")\n",
    "df['grading_date_original'] = df['grading_date'].copy()\n",
    "df['grading_date'] = df['grading_date'].apply(lambda x: clean_date_column(x, 'grading_date'))\n",
    "\n",
    "print(f\"GRADING DATE CLEANING RESULTS:\")\n",
    "print(f\"Original non-null values: {df['grading_date_original'].notna().sum()}\")\n",
    "print(f\"Cleaned non-null values: {df['grading_date'].notna().sum()}\")\n",
    "print(f\"Successfully converted: {df['grading_date'].notna().sum()}\")\n",
    "print(f\"Failed conversions: {df['grading_date_original'].notna().sum() - df['grading_date'].notna().sum()}\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\nGRADING DATE EXAMPLES:\")\n",
    "grading_examples = df[['grading_date_original', 'grading_date']].dropna(subset=['grading_date']).head(10)\n",
    "for idx, row in grading_examples.iterrows():\n",
    "    print(f\"'{row['grading_date_original']}' → {row['grading_date'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. CLEANING EXPIRATION DATE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clean expiration date\n",
    "print(\"Cleaning expiration column...\")\n",
    "df['expiration_original'] = df['expiration'].copy()\n",
    "df['expiration'] = df['expiration'].apply(lambda x: clean_date_column(x, 'expiration'))\n",
    "\n",
    "print(f\"EXPIRATION DATE CLEANING RESULTS:\")\n",
    "print(f\"Original non-null values: {df['expiration_original'].notna().sum()}\")\n",
    "print(f\"Cleaned non-null values: {df['expiration'].notna().sum()}\")\n",
    "print(f\"Successfully converted: {df['expiration'].notna().sum()}\")\n",
    "print(f\"Failed conversions: {df['expiration_original'].notna().sum() - df['expiration'].notna().sum()}\")\n",
    "\n",
    "# Show examples\n",
    "print(f\"\\nEXPIRATION DATE EXAMPLES:\")\n",
    "expiration_examples = df[['expiration_original', 'expiration']].dropna(subset=['expiration']).head(10)\n",
    "for idx, row in expiration_examples.iterrows():\n",
    "    print(f\"'{row['expiration_original']}' → {row['expiration'].strftime('%Y-%m-%d')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. DATA VALIDATION AND SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Validate date relationships\n",
    "print(\"DATE RELATIONSHIP VALIDATION:\")\n",
    "\n",
    "# Check if grading_date < expiration (should be true for valid data)\n",
    "valid_relationships = df[(df['grading_date'].notna()) & (df['expiration'].notna())]\n",
    "correct_order = (valid_relationships['grading_date'] < valid_relationships['expiration']).sum()\n",
    "total_comparable = len(valid_relationships)\n",
    "\n",
    "print(f\"Records with both grading and expiration dates: {total_comparable}\")\n",
    "print(f\"Records where grading < expiration: {correct_order}\")\n",
    "print(f\"Records with date order issues: {total_comparable - correct_order}\")\n",
    "\n",
    "# Check harvest year vs grading date year\n",
    "harvest_grading_compare = df[(df['harvest_year'].notna()) & (df['grading_date'].notna())].copy()\n",
    "harvest_grading_compare['grading_year'] = harvest_grading_compare['grading_date'].dt.year\n",
    "year_diff = harvest_grading_compare['grading_year'] - harvest_grading_compare['harvest_year']\n",
    "\n",
    "print(f\"\\nHARVEST vs GRADING YEAR:\")\n",
    "print(f\"Records with both harvest year and grading date: {len(harvest_grading_compare)}\")\n",
    "print(f\"Average time from harvest to grading: {year_diff.mean():.1f} years\")\n",
    "print(f\"Year difference distribution:\")\n",
    "print(year_diff.value_counts().sort_index().head(10))\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['harvest_year_original', 'grading_date_original', 'expiration_original'], axis=1, inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY AND DOCUMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "DATE CLEANING DOCUMENTATION:\n",
    "\n",
    "1. HARVEST YEAR CLEANING:\n",
    "   - Converted to integer years (int64)\n",
    "   - Handled range formats (2013/2014 → 2013)\n",
    "   - Handled seasonal formats (2017 / 2018 → 2017) \n",
    "   - Handled quarter formats (4T/10 → 2010)\n",
    "   - Removed invalid text entries (Mayo a Julio → NaN)\n",
    "   - Valid range: 1900-2025\n",
    "\n",
    "2. GRADING DATE CLEANING:\n",
    "   - Converted to pandas datetime objects\n",
    "   - Handled ordinal suffixes (4th, 1st, 2nd, 3rd)\n",
    "   - Primary format: \"Month DDth, YYYY\" \n",
    "   - Fallback to multiple date format attempts\n",
    "   - Invalid dates converted to NaT\n",
    "\n",
    "3. EXPIRATION DATE CLEANING:\n",
    "   - Same approach as grading dates\n",
    "   - Converted to pandas datetime objects\n",
    "   - Consistent format handling\n",
    "\n",
    "4. DATA TYPES AFTER CLEANING:\n",
    "   - harvest_year: int64 (or float64 with NaN)\n",
    "   - grading_date: datetime64[ns]\n",
    "   - expiration: datetime64[ns]\n",
    "\n",
    "5. VALIDATION:\n",
    "   - Checked logical date relationships\n",
    "   - Verified harvest-to-grading time patterns\n",
    "   - Maintained data integrity\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Final data types:\")\n",
    "for col in ['harvest_year', 'grading_date', 'expiration']:\n",
    "    print(f\"{col}: {df[col].dtype}\")\n",
    "\n",
    "print(f\"\\nFinal non-null counts:\")\n",
    "for col in ['harvest_year', 'grading_date', 'expiration']:\n",
    "    print(f\"{col}: {df[col].notna().sum()}/{len(df)} ({df[col].notna().sum()/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff33b4-c94d-43b3-bab3-97eabb862a37",
   "metadata": {},
   "source": [
    "### Problem 6. Countries (1 point)\n",
    "How many coffees are there with unknown countries of origin? What can you do about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0851c1a8-0420-4dba-ac27-487bae4318be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COUNTRIES ANALYSIS ===\n",
      "\n",
      "COUNTRY OF ORIGIN ANALYSIS:\n",
      "Total records: 1339\n",
      "Non-null countries: 1338\n",
      "Null/missing countries: 1\n",
      "Empty string countries: 0\n",
      "\n",
      "MISSING COUNTRIES SUMMARY:\n",
      "Total missing: 1 (0.07%)\n",
      "\n",
      "COUNTRY DISTRIBUTION (top 15):\n",
      "country_of_origin\n",
      "Mexico                          236\n",
      "Colombia                        183\n",
      "Guatemala                       181\n",
      "Brazil                          132\n",
      "Taiwan                           75\n",
      "United States (Hawaii)           73\n",
      "Honduras                         53\n",
      "Costa Rica                       51\n",
      "Ethiopia                         44\n",
      "Tanzania, United Republic Of     40\n",
      "Uganda                           36\n",
      "Thailand                         32\n",
      "Nicaragua                        26\n",
      "Kenya                            25\n",
      "El Salvador                      21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total unique countries: 36\n",
      "\n",
      "============================================================\n",
      "ANALYSIS OF MISSING COUNTRY RECORDS\n",
      "============================================================\n",
      "Records with unknown countries: 1\n",
      "\n",
      "Detailed analysis of missing country records:\n",
      "\n",
      "Missing country records details:\n",
      "\n",
      "Record 1197:\n",
      "  owner: 'racafe & cia s.c.a'\n",
      "  company: null/empty\n",
      "  producer: null/empty\n",
      "  region: null/empty\n",
      "  farm_name: null/empty\n",
      "  mill: null/empty\n",
      "  ico_number: '3-37-1980'\n",
      "  harvest_year: null/empty\n",
      "  grading_date: '2011-03-01 00:00:00'\n",
      "  total_cup_points: '79.08'\n",
      "\n",
      "============================================================\n",
      "STRATEGIES FOR HANDLING MISSING COUNTRIES\n",
      "============================================================\n",
      "STRATEGY 1: INFERENCE FROM OTHER FIELDS\n",
      "----------------------------------------\n",
      "Attempting to infer countries from available data...\n",
      "\n",
      "Record 1197:\n",
      "  Available info: ['racafe & cia s.c.a']\n",
      "  Inferred country: Could not infer\n",
      "\n",
      "----------------------------------------\n",
      "STRATEGY 2: ICO NUMBER ANALYSIS\n",
      "----------------------------------------\n",
      "Record 1197 ICO Number: '3-37-1980'\n",
      "  Analysis: ICO number format suggests further research needed\n",
      "\n",
      "============================================================\n",
      "RECOMMENDED ACTIONS\n",
      "============================================================\n",
      "1. IMMEDIATE ACTIONS:\n",
      "   - Document missing data: 1 records (0.07%)\n",
      "   - Keep records with missing countries (data is still valuable)\n",
      "   - Flag records for manual review\n",
      "\n",
      "2. RESEARCH ACTIONS:\n",
      "   - Research ICO number patterns for geographic codes\n",
      "   - Cross-reference owner/company names with coffee industry databases\n",
      "   - Look up certification body addresses for geographic clues\n",
      "\n",
      "3. DATA HANDLING OPTIONS:\n",
      "   Option A: Create 'Unknown' category\n",
      "     - Replace missing values with 'Unknown'\n",
      "     - Preserves all records for analysis\n",
      "     - Clear indication of missing data\n",
      "   Option B: Remove records with missing countries\n",
      "     - Would remove 1 records\n",
      "     - NOT RECOMMENDED: Loss of valuable quality data\n",
      "   Option C: Statistical imputation\n",
      "     - NOT APPROPRIATE: Countries are nominal categorical data\n",
      "     - No meaningful 'average' or 'most common' substitute\n",
      "\n",
      "============================================================\n",
      "IMPLEMENTATION: HANDLING MISSING COUNTRIES\n",
      "============================================================\n",
      "Implementing Option A: Create 'Unknown' category for missing countries\n",
      "RESULTS:\n",
      "- Original null values: 1\n",
      "- Original empty strings: 0\n",
      "- New 'Unknown' entries: 1\n",
      "- Total handled: 1\n",
      "\n",
      "FINAL COUNTRY DISTRIBUTION (top 10):\n",
      "country_of_origin\n",
      "Mexico                          236\n",
      "Colombia                        183\n",
      "Guatemala                       181\n",
      "Brazil                          132\n",
      "Taiwan                           75\n",
      "United States (Hawaii)           73\n",
      "Honduras                         53\n",
      "Costa Rica                       51\n",
      "Ethiopia                         44\n",
      "Tanzania, United Republic Of     40\n",
      "Name: count, dtype: int64\n",
      "\n",
      "'Unknown' countries: 1 (0.07%)\n",
      "\n",
      "✓ Successfully handled missing country data\n",
      "✓ All 1339 records preserved\n",
      "✓ Missing data clearly marked as 'Unknown'\n",
      "\n",
      "============================================================\n",
      "SUMMARY AND DOCUMENTATION\n",
      "============================================================\n",
      "\n",
      "MISSING COUNTRIES ANALYSIS SUMMARY:\n",
      "\n",
      "1. FINDINGS:\n",
      "   - Total records: 1339\n",
      "   - Missing countries: 1 (0.07%)\n",
      "   - Very low missing data rate - excellent data quality\n",
      "\n",
      "2. ROOT CAUSE:\n",
      "   - Appears to be isolated data entry issues\n",
      "   - Not systematic missing data problem\n",
      "\n",
      "3. SOLUTION IMPLEMENTED:\n",
      "   - Replaced missing values with 'Unknown' category\n",
      "   - Preserved all records (valuable quality data)\n",
      "   - Clear documentation of missing data\n",
      "\n",
      "4. ALTERNATIVE APPROACHES CONSIDERED:\n",
      "   - Record removal: Rejected (unnecessary data loss)\n",
      "   - Imputation: Rejected (inappropriate for categorical country data)\n",
      "   - Manual research: Recommended for future data collection\n",
      "\n",
      "5. IMPACT:\n",
      "   - No data loss\n",
      "   - Clear missing data handling\n",
      "   - Ready for geographic analysis with proper missing data acknowledgment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Continue from previous problems - assuming df is already loaded with cleaned data\n",
    "# df = pd.read_csv('merged_data_cleaned.csv', index_col=0)\n",
    "# df.columns = [to_snake_case(col) for col in df.columns]  # from Problem 3\n",
    "\n",
    "print(\"=== COUNTRIES ANALYSIS ===\\n\")\n",
    "\n",
    "# Basic analysis of country_of_origin column\n",
    "print(\"COUNTRY OF ORIGIN ANALYSIS:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Non-null countries: {df['country_of_origin'].notna().sum()}\")\n",
    "print(f\"Null/missing countries: {df['country_of_origin'].isna().sum()}\")\n",
    "print(f\"Empty string countries: {(df['country_of_origin'] == '').sum()}\")\n",
    "\n",
    "# Calculate percentage of missing countries\n",
    "missing_countries = df['country_of_origin'].isna().sum() + (df['country_of_origin'] == '').sum()\n",
    "missing_percentage = (missing_countries / len(df)) * 100\n",
    "\n",
    "print(f\"\\nMISSING COUNTRIES SUMMARY:\")\n",
    "print(f\"Total missing: {missing_countries} ({missing_percentage:.2f}%)\")\n",
    "\n",
    "# Show country distribution\n",
    "print(f\"\\nCOUNTRY DISTRIBUTION (top 15):\")\n",
    "country_counts = df['country_of_origin'].value_counts()\n",
    "print(country_counts.head(15))\n",
    "\n",
    "print(f\"\\nTotal unique countries: {df['country_of_origin'].nunique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS OF MISSING COUNTRY RECORDS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get records with missing countries\n",
    "missing_mask = df['country_of_origin'].isna() | (df['country_of_origin'] == '')\n",
    "missing_records = df[missing_mask].copy()\n",
    "\n",
    "print(f\"Records with unknown countries: {len(missing_records)}\")\n",
    "\n",
    "if len(missing_records) > 0:\n",
    "    print(f\"\\nDetailed analysis of missing country records:\")\n",
    "    \n",
    "    # Show all relevant columns for missing records\n",
    "    relevant_columns = ['owner', 'company', 'producer', 'region', 'farm_name', 'mill', 'ico_number', \n",
    "                       'harvest_year', 'grading_date', 'total_cup_points']\n",
    "    \n",
    "    print(\"\\nMissing country records details:\")\n",
    "    for idx, row in missing_records.iterrows():\n",
    "        print(f\"\\nRecord {idx}:\")\n",
    "        for col in relevant_columns:\n",
    "            if col in df.columns:\n",
    "                value = row[col]\n",
    "                if pd.notna(value) and value != '':\n",
    "                    print(f\"  {col}: '{value}'\")\n",
    "                else:\n",
    "                    print(f\"  {col}: null/empty\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STRATEGIES FOR HANDLING MISSING COUNTRIES\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"STRATEGY 1: INFERENCE FROM OTHER FIELDS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Try to infer from owner/company names\n",
    "    def infer_country_from_text(text_fields):\n",
    "        \"\"\"\n",
    "        Attempt to infer country from text fields using common patterns.\n",
    "        \n",
    "        This function looks for:\n",
    "        1. Country-specific company suffixes (S.A., Ltd., etc.)\n",
    "        2. Geographic indicators in names\n",
    "        3. Known regional coffee organizations\n",
    "        \n",
    "        Returns: inferred country or None\n",
    "        \"\"\"\n",
    "        \n",
    "        if not text_fields or all(pd.isna(field) for field in text_fields):\n",
    "            return None\n",
    "        \n",
    "        # Combine all text fields\n",
    "        combined_text = ' '.join([str(field).lower() for field in text_fields if pd.notna(field)])\n",
    "        \n",
    "        # Country inference patterns\n",
    "        country_patterns = {\n",
    "            'mexico': ['mexican', 'mexico', 'mexicano', 'oaxaca', 'chiapas', 'veracruz'],\n",
    "            'colombia': ['colombian', 'colombia', 'colombiano', 'huila', 'nariño', 'cauca'],\n",
    "            'guatemala': ['guatemalan', 'guatemala', 'guatemalteco', 'antigua', 'huehuetenango'],\n",
    "            'brazil': ['brazilian', 'brazil', 'brasileiro', 'minas gerais', 'sao paulo', 'bahia'],\n",
    "            'costa rica': ['costa rica', 'costarricense', 'tarrazú', 'central valley'],\n",
    "            'honduras': ['honduran', 'honduras', 'hondureño', 'copán', 'santa bárbara'],\n",
    "            'ethiopia': ['ethiopian', 'ethiopia', 'yirgacheffe', 'sidamo', 'harrar'],\n",
    "            'kenya': ['kenyan', 'kenya', 'aa', 'nyeri', 'kirinyaga'],\n",
    "            'nicaragua': ['nicaraguan', 'nicaragua', 'nicaragüense', 'matagalpa', 'jinotega'],\n",
    "            'panama': ['panamanian', 'panama', 'panameño', 'boquete', 'chiriquí'],\n",
    "            'peru': ['peruvian', 'peru', 'peruano', 'chanchamayo', 'cusco'],\n",
    "            'el salvador': ['salvadoran', 'el salvador', 'salvadoreño', 'santa ana', 'ahuachapán']\n",
    "        }\n",
    "        \n",
    "        # Check for patterns\n",
    "        for country, patterns in country_patterns.items():\n",
    "            for pattern in patterns:\n",
    "                if pattern in combined_text:\n",
    "                    return country.title()\n",
    "        \n",
    "        # Check for company suffixes that might indicate country\n",
    "        if 's.a.' in combined_text or 'sociedad anónima' in combined_text:\n",
    "            # Could be Latin American country\n",
    "            return None  # Too ambiguous\n",
    "        \n",
    "        if 'ltd' in combined_text or 'limited' in combined_text:\n",
    "            # Could be English-speaking country, but not specific enough\n",
    "            return None\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # Apply inference to missing records\n",
    "    print(\"Attempting to infer countries from available data...\")\n",
    "    \n",
    "    inferred_countries = []\n",
    "    for idx, row in missing_records.iterrows():\n",
    "        text_fields = [row.get('owner'), row.get('company'), row.get('producer'), \n",
    "                      row.get('region'), row.get('farm_name'), row.get('mill')]\n",
    "        \n",
    "        inferred = infer_country_from_text(text_fields)\n",
    "        inferred_countries.append(inferred)\n",
    "        \n",
    "        print(f\"\\nRecord {idx}:\")\n",
    "        print(f\"  Available info: {[field for field in text_fields if pd.notna(field) and field != '']}\")\n",
    "        print(f\"  Inferred country: {inferred if inferred else 'Could not infer'}\")\n",
    "\n",
    "    # Strategy 2: ICO Number analysis\n",
    "    print(f\"\\n\" + \"-\" * 40)\n",
    "    print(\"STRATEGY 2: ICO NUMBER ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ICO numbers might contain country codes\n",
    "    for idx, row in missing_records.iterrows():\n",
    "        ico_number = row.get('ico_number')\n",
    "        if pd.notna(ico_number) and ico_number != '':\n",
    "            print(f\"Record {idx} ICO Number: '{ico_number}'\")\n",
    "            \n",
    "            # ICO numbers sometimes start with country codes or contain geographic info\n",
    "            # This would require a lookup table of ICO country codes\n",
    "            print(f\"  Analysis: ICO number format suggests further research needed\")\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDED ACTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    actions_taken = []\n",
    "    \n",
    "    print(\"1. IMMEDIATE ACTIONS:\")\n",
    "    print(\"   - Document missing data: {} records ({:.2f}%)\".format(missing_countries, missing_percentage))\n",
    "    print(\"   - Keep records with missing countries (data is still valuable)\")\n",
    "    print(\"   - Flag records for manual review\")\n",
    "    \n",
    "    print(\"\\n2. RESEARCH ACTIONS:\")\n",
    "    print(\"   - Research ICO number patterns for geographic codes\")\n",
    "    print(\"   - Cross-reference owner/company names with coffee industry databases\")\n",
    "    print(\"   - Look up certification body addresses for geographic clues\")\n",
    "    \n",
    "    print(\"\\n3. DATA HANDLING OPTIONS:\")\n",
    "    \n",
    "    # Option A: Create 'Unknown' category\n",
    "    print(\"   Option A: Create 'Unknown' category\")\n",
    "    df_option_a = df.copy()\n",
    "    df_option_a['country_of_origin'] = df_option_a['country_of_origin'].fillna('Unknown')\n",
    "    df_option_a['country_of_origin'] = df_option_a['country_of_origin'].replace('', 'Unknown')\n",
    "    \n",
    "    print(f\"     - Replace missing values with 'Unknown'\")\n",
    "    print(f\"     - Preserves all records for analysis\")\n",
    "    print(f\"     - Clear indication of missing data\")\n",
    "    \n",
    "    # Option B: Remove records (not recommended for such few records)\n",
    "    print(\"   Option B: Remove records with missing countries\")\n",
    "    print(f\"     - Would remove {missing_countries} records\")\n",
    "    print(f\"     - NOT RECOMMENDED: Loss of valuable quality data\")\n",
    "    \n",
    "    # Option C: Statistical imputation (not appropriate for categorical country data)\n",
    "    print(\"   Option C: Statistical imputation\")\n",
    "    print(\"     - NOT APPROPRIATE: Countries are nominal categorical data\")\n",
    "    print(\"     - No meaningful 'average' or 'most common' substitute\")\n",
    "\n",
    "else:\n",
    "    print(\"✓ Great! No records with missing countries found.\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLEMENTATION: HANDLING MISSING COUNTRIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Implement the recommended approach\n",
    "print(\"Implementing Option A: Create 'Unknown' category for missing countries\")\n",
    "\n",
    "# Handle missing countries\n",
    "original_nulls = df['country_of_origin'].isna().sum()\n",
    "original_empty = (df['country_of_origin'] == '').sum()\n",
    "\n",
    "# Replace missing values with 'Unknown'\n",
    "df['country_of_origin'] = df['country_of_origin'].fillna('Unknown')\n",
    "df['country_of_origin'] = df['country_of_origin'].replace('', 'Unknown')\n",
    "\n",
    "# Verify changes\n",
    "new_unknown = (df['country_of_origin'] == 'Unknown').sum()\n",
    "\n",
    "print(f\"RESULTS:\")\n",
    "print(f\"- Original null values: {original_nulls}\")\n",
    "print(f\"- Original empty strings: {original_empty}\")\n",
    "print(f\"- New 'Unknown' entries: {new_unknown}\")\n",
    "print(f\"- Total handled: {original_nulls + original_empty}\")\n",
    "\n",
    "# Final country distribution\n",
    "print(f\"\\nFINAL COUNTRY DISTRIBUTION (top 10):\")\n",
    "final_country_counts = df['country_of_origin'].value_counts()\n",
    "print(final_country_counts.head(10))\n",
    "\n",
    "if new_unknown > 0:\n",
    "    print(f\"\\n'Unknown' countries: {new_unknown} ({(new_unknown/len(df)*100):.2f}%)\")\n",
    "\n",
    "print(f\"\\n✓ Successfully handled missing country data\")\n",
    "print(f\"✓ All {len(df)} records preserved\")\n",
    "print(f\"✓ Missing data clearly marked as 'Unknown'\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY AND DOCUMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "MISSING COUNTRIES ANALYSIS SUMMARY:\n",
    "\n",
    "1. FINDINGS:\n",
    "   - Total records: {len(df)}\n",
    "   - Missing countries: {original_nulls + original_empty} ({((original_nulls + original_empty)/len(df)*100):.2f}%)\n",
    "   - Very low missing data rate - excellent data quality\n",
    "\n",
    "2. ROOT CAUSE:\n",
    "   - Appears to be isolated data entry issues\n",
    "   - Not systematic missing data problem\n",
    "\n",
    "3. SOLUTION IMPLEMENTED:\n",
    "   - Replaced missing values with 'Unknown' category\n",
    "   - Preserved all records (valuable quality data)\n",
    "   - Clear documentation of missing data\n",
    "\n",
    "4. ALTERNATIVE APPROACHES CONSIDERED:\n",
    "   - Record removal: Rejected (unnecessary data loss)\n",
    "   - Imputation: Rejected (inappropriate for categorical country data)\n",
    "   - Manual research: Recommended for future data collection\n",
    "\n",
    "5. IMPACT:\n",
    "   - No data loss\n",
    "   - Clear missing data handling\n",
    "   - Ready for geographic analysis with proper missing data acknowledgment\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aa6f30-4b93-4f23-95e0-2cafb7152c6c",
   "metadata": {},
   "source": [
    "### Problem 7. Owners (1 point)\n",
    "There are two suspicious columns, named `Owner`, and `Owner.1` (they're likely called something different after you solved problem 3). Do something about them. Is there any link to `Producer`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39bc1689-33f6-4446-bfc3-c4b4e69eccf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OWNERS COLUMN ANALYSIS ===\n",
      "\n",
      "Available owner-related columns: ['owner', 'owner_1', 'producer']\n",
      "Working with columns: owner, owner_1, producer\n",
      "\n",
      "============================================================\n",
      "BASIC STATISTICS\n",
      "============================================================\n",
      "Total records: 1339\n",
      "owner non-null: 1332\n",
      "owner_1 non-null: 1332\n",
      "producer non-null: 1107\n",
      "\n",
      "Unique values:\n",
      "owner: 315\n",
      "owner_1: 319\n",
      "producer: 692\n",
      "\n",
      "============================================================\n",
      "COLUMN RELATIONSHIP ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Owner vs Owner.1:\n",
      "  Records with both values: 1332\n",
      "  Exact matches: 3 (0.2%)\n",
      "  Case-insensitive matches: 1328 (99.7%)\n",
      "  Only case/whitespace differences: 1325\n",
      "  Examples of substantive differences:\n",
      "    'ceca, s.a.' vs 'CECA,S.A.'\n",
      "    'federacion nacional de cafeteros' vs 'Federación Nacional de Cafeteros'\n",
      "    'klem organics' vs 'KlemOrganics'\n",
      "    'klem organics' vs 'KlemOrganics'\n",
      "\n",
      "Owner vs Producer:\n",
      "  Records with both values: 1100\n",
      "  Exact matches: 0 (0.0%)\n",
      "  Case-insensitive matches: 151 (13.7%)\n",
      "  Only case/whitespace differences: 151\n",
      "  Examples of substantive differences:\n",
      "    'yidnekachew dabessa' vs 'Yidnekachew Dabessa Coffee Plantation'\n",
      "    'hugo valdivia' vs 'HVC'\n",
      "    'ethiopia commodity exchange' vs 'Bazen Agricultural & Industrial Dev't Plc'\n",
      "    'ethiopia commodity exchange' vs 'Bazen Agricultural & Industrial Dev't Plc'\n",
      "    'mohammed lalo' vs 'Fahem Coffee Plantation'\n",
      "\n",
      "Owner.1 vs Producer:\n",
      "  Records with both values: 1100\n",
      "  Exact matches: 129 (11.7%)\n",
      "  Case-insensitive matches: 151 (13.7%)\n",
      "  Only case/whitespace differences: 22\n",
      "  Examples of substantive differences:\n",
      "    'Yidnekachew Dabessa' vs 'Yidnekachew Dabessa Coffee Plantation'\n",
      "    'Hugo Valdivia' vs 'HVC'\n",
      "    'Ethiopia Commodity Exchange' vs 'Bazen Agricultural & Industrial Dev't Plc'\n",
      "    'Ethiopia Commodity Exchange' vs 'Bazen Agricultural & Industrial Dev't Plc'\n",
      "    'Mohammed Lalo' vs 'Fahem Coffee Plantation'\n",
      "\n",
      "============================================================\n",
      "DETAILED ANALYSIS OF OWNER vs OWNER.1\n",
      "============================================================\n",
      "Records where Owner == Owner.1 exactly: 3\n",
      "Records with only case/formatting differences: 1325\n",
      "Examples of case/formatting differences:\n",
      "  'grounds for health admin' → 'Grounds for Health Admin'\n",
      "  'yidnekachew dabessa' → 'Yidnekachew Dabessa'\n",
      "  'ji-ae ahn' → 'Ji-Ae Ahn'\n",
      "  'hugo valdivia' → 'Hugo Valdivia'\n",
      "  'ethiopia commodity exchange' → 'Ethiopia Commodity Exchange'\n",
      "  'ethiopia commodity exchange' → 'Ethiopia Commodity Exchange'\n",
      "  'diamond enterprise plc' → 'Diamond Enterprise Plc'\n",
      "  'mohammed lalo' → 'Mohammed Lalo'\n",
      "  'cqi q coffee sample representative' → 'CQI Q Coffee Sample Representative'\n",
      "  'cqi q coffee sample representative' → 'CQI Q Coffee Sample Representative'\n",
      "\n",
      "Records with substantive differences: 4\n",
      "Examples of substantive differences:\n",
      "  'ceca, s.a.' vs 'CECA,S.A.'\n",
      "  'federacion nacional de cafeteros' vs 'Federación Nacional de Cafeteros'\n",
      "  'klem organics' vs 'KlemOrganics'\n",
      "  'klem organics' vs 'KlemOrganics'\n",
      "\n",
      "============================================================\n",
      "ANALYSIS CONCLUSIONS\n",
      "============================================================\n",
      "\n",
      "FINDINGS:\n",
      "\n",
      "1. OWNER vs OWNER.1 RELATIONSHIP:\n",
      "   - Total records with both values: 1332\n",
      "   - Identical content (case-insensitive): 1328 (99.7%)\n",
      "   - Only formatting differences: 1325\n",
      "   - Substantive differences: 4\n",
      "\n",
      "2. INTERPRETATION:\n",
      "   - Owner and Owner.1 appear to be DUPLICATE columns\n",
      "   - Owner.1 seems to have cleaner formatting (proper capitalization)\n",
      "   - Very few substantive differences suggest data entry inconsistencies\n",
      "\n",
      "3. PRODUCER RELATIONSHIP:\n",
      "   - Producer is a DIFFERENT entity from Owner/Owner.1\n",
      "   - Producer has more unique values (different granularity)\n",
      "   - Some overlap but serves different business purpose\n",
      "\n",
      "\n",
      "============================================================\n",
      "RECOMMENDED ACTIONS\n",
      "============================================================\n",
      "STRATEGY: Consolidate Owner columns and clean Producer\n",
      "Consolidating Owner and Owner.1 into single 'owner' column...\n",
      "\n",
      "CONSOLIDATION RESULTS:\n",
      "Original Owner non-null: 1332\n",
      "Original Owner.1 non-null: 1332\n",
      "Consolidated owner non-null: 1332\n",
      "Unique values in consolidated: 318\n",
      "\n",
      "EXAMPLES OF CONSOLIDATION:\n",
      "'grounds for health admin' + 'Grounds for Health Admin' → 'Grounds for Health Admin'\n",
      "'yidnekachew dabessa' + 'Yidnekachew Dabessa' → 'Yidnekachew Dabessa'\n",
      "'ji-ae ahn' + 'Ji-Ae Ahn' → 'Ji-Ae Ahn'\n",
      "'hugo valdivia' + 'Hugo Valdivia' → 'Hugo Valdivia'\n",
      "'ethiopia commodity exchange' + 'Ethiopia Commodity Exchange' → 'Ethiopia Commodity Exchange'\n",
      "'ethiopia commodity exchange' + 'Ethiopia Commodity Exchange' → 'Ethiopia Commodity Exchange'\n",
      "'diamond enterprise plc' + 'Diamond Enterprise Plc' → 'Diamond Enterprise Plc'\n",
      "\n",
      "============================================================\n",
      "IMPLEMENTING CHANGES\n",
      "============================================================\n",
      "✓ Consolidated owner and owner_1\n",
      "✓ Removed redundant owner_1 column\n",
      "✓ Kept backup columns for reference\n",
      "\n",
      "PRODUCER COLUMN STATUS:\n",
      "Producer remains separate (different business entity)\n",
      "Producer non-null values: 1107\n",
      "Producer unique values: 692\n",
      "\n",
      "============================================================\n",
      "FINAL VERIFICATION\n",
      "============================================================\n",
      "Columns after cleanup:\n",
      "- owner: 1332 non-null values\n",
      "- producer: 1107 non-null values\n",
      "Records where Owner = Producer: 151\n",
      "This is normal - sometimes the same entity owns and produces\n",
      "\n",
      "✓ Owner columns successfully consolidated\n",
      "✓ Data quality improved through better formatting\n",
      "✓ Producer column maintained as separate business entity\n",
      "\n",
      "============================================================\n",
      "DOCUMENTATION OF CHANGES\n",
      "============================================================\n",
      "\n",
      "OWNER COLUMNS CLEANUP SUMMARY:\n",
      "\n",
      "1. PROBLEM IDENTIFIED:\n",
      "   - Two suspicious columns: Owner and Owner.1\n",
      "   - High correlation (99%+ same content)\n",
      "   - Owner.1 had better formatting/capitalization\n",
      "   - Redundant information causing data duplication\n",
      "\n",
      "2. SOLUTION IMPLEMENTED:\n",
      "   - Consolidated into single 'owner' column\n",
      "   - Chose better-formatted values (preferring Owner.1)\n",
      "   - Handled missing values appropriately\n",
      "   - Kept backup columns for audit trail\n",
      "\n",
      "3. PRODUCER RELATIONSHIP:\n",
      "   - Producer is SEPARATE from Owner (different business role)\n",
      "   - Owner: Legal entity that owns the coffee/business\n",
      "   - Producer: Entity that grows/produces the coffee\n",
      "   - Sometimes same, sometimes different (valid business scenarios)\n",
      "\n",
      "4. RESULTS:\n",
      "   - Eliminated redundant column\n",
      "   - Improved data quality through better formatting\n",
      "   - Maintained all original information\n",
      "\n",
      "5. DATA TYPES MAINTAINED:\n",
      "   - owner: object (string) - consolidated entity name\n",
      "   - producer: object (string) - separate producer entity\n",
      "   \n",
      "6. BUSINESS LOGIC:\n",
      "   - Owner = Legal owner/buyer of the coffee\n",
      "   - Producer = Farm/entity that grows the coffee  \n",
      "   - Can be same entity (vertical integration)\n",
      "   - Can be different (supply chain separation)\n",
      "\n",
      "\n",
      "FINAL SAMPLE DATA:\n",
      "                         owner                                   producer country_of_origin  total_cup_points\n",
      "0                    metad plc                                  METAD PLC          Ethiopia             90.58\n",
      "1                    metad plc                                  METAD PLC          Ethiopia             89.92\n",
      "2     Grounds for Health Admin                                        NaN         Guatemala             89.75\n",
      "3          Yidnekachew Dabessa      Yidnekachew Dabessa Coffee Plantation          Ethiopia             89.00\n",
      "4                    metad plc                                  METAD PLC          Ethiopia             88.83\n",
      "5                    Ji-Ae Ahn                                        NaN            Brazil             88.83\n",
      "6                Hugo Valdivia                                        HVC              Peru             88.75\n",
      "7  Ethiopia Commodity Exchange  Bazen Agricultural & Industrial Dev't Plc          Ethiopia             88.67\n",
      "8  Ethiopia Commodity Exchange  Bazen Agricultural & Industrial Dev't Plc          Ethiopia             88.42\n",
      "9       Diamond Enterprise Plc                     Diamond Enterprise Plc          Ethiopia             88.25\n",
      "\n",
      "============================================================\n",
      "DATA INTEGRITY VERIFICATION\n",
      "============================================================\n",
      "Data integrity check:\n",
      "- Original Owner records: 1332\n",
      "- Original Owner.1 records: 1332\n",
      "- Final consolidated records: 1332\n",
      "- Data preserved: True\n",
      "✓ All data successfully preserved\n",
      "\n",
      "Consolidation decision statistics:\n",
      "- Records where both columns were identical: 1328\n",
      "- Times chose original Owner: 4\n",
      "- Times chose Owner.1: 0\n",
      "\n",
      "✓ Owner column consolidation completed successfully!\n",
      "✓ Ready for further analysis with clean, non-redundant data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Continue from previous problems - assuming df is already loaded with snake_case columns\n",
    "# df = pd.read_csv('merged_data_cleaned.csv', index_col=0)\n",
    "# df.columns = [to_snake_case(col) for col in df.columns]  # from Problem 3\n",
    "\n",
    "print(\"=== OWNERS COLUMN ANALYSIS ===\\n\")\n",
    "\n",
    "# After Problem 3, these should be snake_case: owner, owner_1, producer\n",
    "owner_cols = ['owner', 'owner_1', 'producer']\n",
    "\n",
    "# Verify columns exist (handle both cases)\n",
    "available_cols = []\n",
    "for col in owner_cols:\n",
    "    if col in df.columns:\n",
    "        available_cols.append(col)\n",
    "    else:\n",
    "        # Try original format if snake_case not applied yet\n",
    "        original_mappings = {'owner': 'Owner', 'owner_1': 'Owner.1', 'producer': 'Producer'}\n",
    "        original_col = original_mappings.get(col)\n",
    "        if original_col in df.columns:\n",
    "            available_cols.append(original_col)\n",
    "\n",
    "print(f\"Available owner-related columns: {available_cols}\")\n",
    "\n",
    "# For this analysis, let's work with the actual column names\n",
    "owner_col = 'owner' if 'owner' in df.columns else 'Owner'\n",
    "owner1_col = 'owner_1' if 'owner_1' in df.columns else 'Owner.1'\n",
    "producer_col = 'producer' if 'producer' in df.columns else 'Producer'\n",
    "\n",
    "print(f\"Working with columns: {owner_col}, {owner1_col}, {producer_col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"{owner_col} non-null: {df[owner_col].notna().sum()}\")\n",
    "print(f\"{owner1_col} non-null: {df[owner1_col].notna().sum()}\")\n",
    "print(f\"{producer_col} non-null: {df[producer_col].notna().sum()}\")\n",
    "\n",
    "print(f\"\\nUnique values:\")\n",
    "print(f\"{owner_col}: {df[owner_col].nunique()}\")\n",
    "print(f\"{owner1_col}: {df[owner1_col].nunique()}\")\n",
    "print(f\"{producer_col}: {df[producer_col].nunique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN RELATIONSHIP ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Function to compare two columns\n",
    "def compare_columns(col1, col2, col1_name, col2_name):\n",
    "    \"\"\"Compare two columns for exact matches, case differences, etc.\"\"\"\n",
    "    \n",
    "    # Get non-null pairs\n",
    "    mask = df[col1].notna() & df[col2].notna()\n",
    "    pairs = df[mask][[col1, col2]].copy()\n",
    "    \n",
    "    if len(pairs) == 0:\n",
    "        print(f\"No records with both {col1_name} and {col2_name}\")\n",
    "        return\n",
    "    \n",
    "    # Exact matches\n",
    "    exact_matches = (pairs[col1] == pairs[col2]).sum()\n",
    "    \n",
    "    # Case-insensitive matches\n",
    "    case_insensitive_matches = (\n",
    "        pairs[col1].str.lower().str.strip() == \n",
    "        pairs[col2].str.lower().str.strip()\n",
    "    ).sum()\n",
    "    \n",
    "    print(f\"\\n{col1_name} vs {col2_name}:\")\n",
    "    print(f\"  Records with both values: {len(pairs)}\")\n",
    "    print(f\"  Exact matches: {exact_matches} ({exact_matches/len(pairs)*100:.1f}%)\")\n",
    "    print(f\"  Case-insensitive matches: {case_insensitive_matches} ({case_insensitive_matches/len(pairs)*100:.1f}%)\")\n",
    "    print(f\"  Only case/whitespace differences: {case_insensitive_matches - exact_matches}\")\n",
    "    \n",
    "    # Show examples of differences\n",
    "    differences = pairs[\n",
    "        pairs[col1].str.lower().str.strip() != pairs[col2].str.lower().str.strip()\n",
    "    ].head(5)\n",
    "    \n",
    "    if len(differences) > 0:\n",
    "        print(f\"  Examples of substantive differences:\")\n",
    "        for idx, row in differences.iterrows():\n",
    "            print(f\"    '{row[col1]}' vs '{row[col2]}'\")\n",
    "    \n",
    "    return exact_matches, case_insensitive_matches\n",
    "\n",
    "# Compare all pairs\n",
    "owner_vs_owner1 = compare_columns(owner_col, owner1_col, \"Owner\", \"Owner.1\")\n",
    "owner_vs_producer = compare_columns(owner_col, producer_col, \"Owner\", \"Producer\")\n",
    "owner1_vs_producer = compare_columns(owner1_col, producer_col, \"Owner.1\", \"Producer\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED ANALYSIS OF OWNER vs OWNER.1\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Deep dive into Owner vs Owner.1 differences\n",
    "mask_both_owners = df[owner_col].notna() & df[owner1_col].notna()\n",
    "owner_pairs = df[mask_both_owners][[owner_col, owner1_col]].copy()\n",
    "\n",
    "# Find exact duplicates\n",
    "exact_same = owner_pairs[owner_pairs[owner_col] == owner_pairs[owner1_col]]\n",
    "print(f\"Records where Owner == Owner.1 exactly: {len(exact_same)}\")\n",
    "\n",
    "# Find case/formatting differences only\n",
    "case_only_diff = owner_pairs[\n",
    "    (owner_pairs[owner_col].str.lower().str.strip() == \n",
    "     owner_pairs[owner1_col].str.lower().str.strip()) &\n",
    "    (owner_pairs[owner_col] != owner_pairs[owner1_col])\n",
    "]\n",
    "\n",
    "print(f\"Records with only case/formatting differences: {len(case_only_diff)}\")\n",
    "\n",
    "if len(case_only_diff) > 0:\n",
    "    print(\"Examples of case/formatting differences:\")\n",
    "    for idx, row in case_only_diff.head(10).iterrows():\n",
    "        print(f\"  '{row[owner_col]}' → '{row[owner1_col]}'\")\n",
    "\n",
    "# Find substantive differences\n",
    "substantive_diff = owner_pairs[\n",
    "    owner_pairs[owner_col].str.lower().str.strip() != \n",
    "    owner_pairs[owner1_col].str.lower().str.strip()\n",
    "]\n",
    "\n",
    "print(f\"\\nRecords with substantive differences: {len(substantive_diff)}\")\n",
    "\n",
    "if len(substantive_diff) > 0:\n",
    "    print(\"Examples of substantive differences:\")\n",
    "    for idx, row in substantive_diff.head(10).iterrows():\n",
    "        print(f\"  '{row[owner_col]}' vs '{row[owner1_col]}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS CONCLUSIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_owner_pairs = len(owner_pairs)\n",
    "case_insensitive_same = len(exact_same) + len(case_only_diff)\n",
    "\n",
    "print(f\"\"\"\n",
    "FINDINGS:\n",
    "\n",
    "1. OWNER vs OWNER.1 RELATIONSHIP:\n",
    "   - Total records with both values: {total_owner_pairs}\n",
    "   - Identical content (case-insensitive): {case_insensitive_same} ({case_insensitive_same/total_owner_pairs*100:.1f}%)\n",
    "   - Only formatting differences: {len(case_only_diff)}\n",
    "   - Substantive differences: {len(substantive_diff)}\n",
    "\n",
    "2. INTERPRETATION:\n",
    "   - Owner and Owner.1 appear to be DUPLICATE columns\n",
    "   - Owner.1 seems to have cleaner formatting (proper capitalization)\n",
    "   - Very few substantive differences suggest data entry inconsistencies\n",
    "\n",
    "3. PRODUCER RELATIONSHIP:\n",
    "   - Producer is a DIFFERENT entity from Owner/Owner.1\n",
    "   - Producer has more unique values (different granularity)\n",
    "   - Some overlap but serves different business purpose\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDED ACTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"STRATEGY: Consolidate Owner columns and clean Producer\")\n",
    "\n",
    "# Strategy 1: Choose the better formatted column\n",
    "def choose_better_owner(row):\n",
    "    \"\"\"\n",
    "    Choose the better formatted owner value between owner and owner_1.\n",
    "    \n",
    "    Preference:\n",
    "    1. owner_1 if it has proper capitalization\n",
    "    2. owner if owner_1 is null\n",
    "    3. Longer/more complete value if both exist\n",
    "    \"\"\"\n",
    "    \n",
    "    owner = row[owner_col]\n",
    "    owner1 = row[owner1_col]\n",
    "    \n",
    "    # If only one exists, use that\n",
    "    if pd.isna(owner) and pd.notna(owner1):\n",
    "        return owner1\n",
    "    if pd.notna(owner) and pd.isna(owner1):\n",
    "        return owner\n",
    "    if pd.isna(owner) and pd.isna(owner1):\n",
    "        return np.nan\n",
    "    \n",
    "    # Both exist - choose based on quality\n",
    "    owner_str = str(owner).strip()\n",
    "    owner1_str = str(owner1).strip()\n",
    "    \n",
    "    # If case-insensitive same, prefer owner1 (usually better formatted)\n",
    "    if owner_str.lower() == owner1_str.lower():\n",
    "        # Check for proper capitalization in owner1\n",
    "        if owner1_str != owner1_str.lower() and owner1_str != owner1_str.upper():\n",
    "            return owner1  # owner1 has mixed case (likely better formatted)\n",
    "        else:\n",
    "            return owner   # Fall back to original\n",
    "    \n",
    "    # If different, choose the longer/more complete one\n",
    "    if len(owner1_str) > len(owner_str):\n",
    "        return owner1\n",
    "    else:\n",
    "        return owner\n",
    "\n",
    "# Apply the consolidation\n",
    "print(\"Consolidating Owner and Owner.1 into single 'owner' column...\")\n",
    "\n",
    "df['owner_consolidated'] = df.apply(choose_better_owner, axis=1)\n",
    "\n",
    "# Show results\n",
    "print(f\"\\nCONSOLIDATION RESULTS:\")\n",
    "print(f\"Original Owner non-null: {df[owner_col].notna().sum()}\")\n",
    "print(f\"Original Owner.1 non-null: {df[owner1_col].notna().sum()}\")\n",
    "print(f\"Consolidated owner non-null: {df['owner_consolidated'].notna().sum()}\")\n",
    "print(f\"Unique values in consolidated: {df['owner_consolidated'].nunique()}\")\n",
    "\n",
    "# Show examples of consolidation choices\n",
    "print(f\"\\nEXAMPLES OF CONSOLIDATION:\")\n",
    "comparison_sample = df[[owner_col, owner1_col, 'owner_consolidated']].dropna().head(10)\n",
    "for idx, row in comparison_sample.iterrows():\n",
    "    if row[owner_col] != row[owner1_col]:\n",
    "        print(f\"'{row[owner_col]}' + '{row[owner1_col]}' → '{row['owner_consolidated']}'\")\n",
    "\n",
    "# Replace original columns\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLEMENTING CHANGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Backup original columns (for documentation)\n",
    "df[f'{owner_col}_original'] = df[owner_col].copy()\n",
    "df[f'{owner1_col}_original'] = df[owner1_col].copy()\n",
    "\n",
    "# Replace with consolidated version\n",
    "df[owner_col] = df['owner_consolidated']\n",
    "\n",
    "# Drop the redundant column\n",
    "df.drop([owner1_col, 'owner_consolidated'], axis=1, inplace=True)\n",
    "\n",
    "print(f\"✓ Consolidated {owner_col} and {owner1_col}\")\n",
    "print(f\"✓ Removed redundant {owner1_col} column\")\n",
    "print(f\"✓ Kept backup columns for reference\")\n",
    "\n",
    "# Producer analysis\n",
    "print(f\"\\nPRODUCER COLUMN STATUS:\")\n",
    "print(f\"Producer remains separate (different business entity)\")\n",
    "print(f\"Producer non-null values: {df[producer_col].notna().sum()}\")\n",
    "print(f\"Producer unique values: {df[producer_col].nunique()}\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Columns after cleanup:\")\n",
    "print(f\"- {owner_col}: {df[owner_col].notna().sum()} non-null values\")\n",
    "print(f\"- {producer_col}: {df[producer_col].notna().sum()} non-null values\")\n",
    "\n",
    "# Check for any remaining issues\n",
    "owner_producer_same = df[\n",
    "    df[owner_col].notna() & df[producer_col].notna() &\n",
    "    (df[owner_col].str.lower().str.strip() == df[producer_col].str.lower().str.strip())\n",
    "].shape[0]\n",
    "\n",
    "print(f\"Records where Owner = Producer: {owner_producer_same}\")\n",
    "print(f\"This is normal - sometimes the same entity owns and produces\")\n",
    "\n",
    "print(f\"\\n✓ Owner columns successfully consolidated\")\n",
    "print(f\"✓ Data quality improved through better formatting\")\n",
    "print(f\"✓ Producer column maintained as separate business entity\")\n",
    "\n",
    "# Documentation\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DOCUMENTATION OF CHANGES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "OWNER COLUMNS CLEANUP SUMMARY:\n",
    "\n",
    "1. PROBLEM IDENTIFIED:\n",
    "   - Two suspicious columns: Owner and Owner.1\n",
    "   - High correlation (99%+ same content)\n",
    "   - Owner.1 had better formatting/capitalization\n",
    "   - Redundant information causing data duplication\n",
    "\n",
    "2. SOLUTION IMPLEMENTED:\n",
    "   - Consolidated into single 'owner' column\n",
    "   - Chose better-formatted values (preferring Owner.1)\n",
    "   - Handled missing values appropriately\n",
    "   - Kept backup columns for audit trail\n",
    "\n",
    "3. PRODUCER RELATIONSHIP:\n",
    "   - Producer is SEPARATE from Owner (different business role)\n",
    "   - Owner: Legal entity that owns the coffee/business\n",
    "   - Producer: Entity that grows/produces the coffee\n",
    "   - Sometimes same, sometimes different (valid business scenarios)\n",
    "\n",
    "4. RESULTS:\n",
    "   - Eliminated redundant column\n",
    "   - Improved data quality through better formatting\n",
    "   - Maintained all original information\n",
    "\n",
    "5. DATA TYPES MAINTAINED:\n",
    "   - owner: object (string) - consolidated entity name\n",
    "   - producer: object (string) - separate producer entity\n",
    "   \n",
    "6. BUSINESS LOGIC:\n",
    "   - Owner = Legal owner/buyer of the coffee\n",
    "   - Producer = Farm/entity that grows the coffee  \n",
    "   - Can be same entity (vertical integration)\n",
    "   - Can be different (supply chain separation)\n",
    "\"\"\")\n",
    "\n",
    "# Show final sample of cleaned data\n",
    "print(\"\\nFINAL SAMPLE DATA:\")\n",
    "sample_cols = [owner_col, producer_col, 'country_of_origin', 'total_cup_points']\n",
    "available_sample_cols = [col for col in sample_cols if col in df.columns]\n",
    "\n",
    "if available_sample_cols:\n",
    "    sample_data = df[available_sample_cols].head(10)\n",
    "    print(sample_data.to_string())\n",
    "else:\n",
    "    print(\"Sample columns not all available - showing available owner data:\")\n",
    "    print(df[[owner_col, producer_col]].head(10).to_string())\n",
    "\n",
    "# Data integrity checks\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DATA INTEGRITY VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for data loss\n",
    "original_owner_count = df[f'{owner_col}_original'].notna().sum()\n",
    "original_owner1_count = df[f'{owner1_col}_original'].notna().sum()\n",
    "final_owner_count = df[owner_col].notna().sum()\n",
    "\n",
    "print(f\"Data integrity check:\")\n",
    "print(f\"- Original Owner records: {original_owner_count}\")\n",
    "print(f\"- Original Owner.1 records: {original_owner1_count}\")\n",
    "print(f\"- Final consolidated records: {final_owner_count}\")\n",
    "print(f\"- Data preserved: {final_owner_count >= max(original_owner_count, original_owner1_count)}\")\n",
    "\n",
    "# Check for any unexpected nulls\n",
    "new_nulls = final_owner_count < max(original_owner_count, original_owner1_count)\n",
    "if new_nulls:\n",
    "    print(f\"⚠️ Warning: Some data may have been lost during consolidation\")\n",
    "else:\n",
    "    print(f\"✓ All data successfully preserved\")\n",
    "\n",
    "# Show statistics on consolidation choices\n",
    "chose_owner = 0\n",
    "chose_owner1 = 0\n",
    "both_same = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    orig_owner = row[f'{owner_col}_original']\n",
    "    orig_owner1 = row[f'{owner1_col}_original']\n",
    "    final_owner = row[owner_col]\n",
    "    \n",
    "    if pd.notna(orig_owner) and pd.notna(orig_owner1):\n",
    "        if str(orig_owner).strip().lower() == str(orig_owner1).strip().lower():\n",
    "            both_same += 1\n",
    "        elif final_owner == orig_owner:\n",
    "            chose_owner += 1\n",
    "        elif final_owner == orig_owner1:\n",
    "            chose_owner1 += 1\n",
    "\n",
    "print(f\"\\nConsolidation decision statistics:\")\n",
    "print(f\"- Records where both columns were identical: {both_same}\")\n",
    "print(f\"- Times chose original Owner: {chose_owner}\")\n",
    "print(f\"- Times chose Owner.1: {chose_owner1}\")\n",
    "\n",
    "print(f\"\\n✓ Owner column consolidation completed successfully!\")\n",
    "print(f\"✓ Ready for further analysis with clean, non-redundant data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c53923-1b72-4500-af0a-47fdca9f57e9",
   "metadata": {},
   "source": [
    "### Problem 8. Coffee color by country and continent (1 point)\n",
    "Create a table which shows how many coffees of each color are there in every country. Leave the missing values as they are.\n",
    "\n",
    "**Note:** If you ask me, countries should be in rows, I prefer long tables much better than wide ones.\n",
    "\n",
    "Now do the same for continents. You know what continent each country is located in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cbe6a9-ca71-4826-806d-562bc30b40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Continue from previous problems - assuming df is already loaded and cleaned\n",
    "# df = pd.read_csv('merged_data_cleaned.csv', index_col=0)\n",
    "# Previous cleaning steps applied...\n",
    "\n",
    "print(\"=== COFFEE COLOR BY COUNTRY AND CONTINENT ===\\n\")\n",
    "\n",
    "print(\"PART 1: COFFEE COLOR BY COUNTRY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# First, let's examine the color column\n",
    "print(\"Color column analysis:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Non-null colors: {df['color'].notna().sum()}\")\n",
    "print(f\"Null colors: {df['color'].isna().sum()}\")\n",
    "\n",
    "print(f\"\\nUnique colors:\")\n",
    "color_counts = df['color'].value_counts(dropna=False)\n",
    "print(color_counts)\n",
    "\n",
    "# Create country vs color crosstab (countries in rows, colors in columns)\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"COUNTRY vs COLOR TABLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create the crosstab with countries in rows (long format preferred)\n",
    "country_color_table = pd.crosstab(\n",
    "    df['country_of_origin'], \n",
    "    df['color'], \n",
    "    margins=True,\n",
    "    margins_name='Total',\n",
    "    dropna=False  # Keep missing values as requested\n",
    ")\n",
    "\n",
    "print(\"Coffee color distribution by country:\")\n",
    "print(\"(Countries in rows, colors in columns)\")\n",
    "print()\n",
    "print(country_color_table.to_string())\n",
    "\n",
    "# Show summary statistics\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"COUNTRY COLOR SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Total countries: {len(country_color_table) - 1}\")  # -1 for Total row\n",
    "print(f\"Total unique colors: {len(country_color_table.columns) - 1}\")  # -1 for Total column\n",
    "\n",
    "# Show countries with most diverse colors\n",
    "print(f\"\\nCountries with most color diversity:\")\n",
    "color_diversity = (country_color_table > 0).sum(axis=1).sort_values(ascending=False)\n",
    "# Exclude 'Total' row and show top 10\n",
    "color_diversity_top = color_diversity[color_diversity.index != 'Total'].head(10)\n",
    "for country, num_colors in color_diversity_top.items():\n",
    "    print(f\"{country}: {num_colors} different colors\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"PART 2: COFFEE COLOR BY CONTINENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define continent mapping for coffee-producing countries\n",
    "continent_mapping = {\n",
    "    # North America\n",
    "    'Mexico': 'North America',\n",
    "    'United States (Hawaii)': 'North America',\n",
    "    'Guatemala': 'North America',\n",
    "    'Honduras': 'North America',\n",
    "    'Nicaragua': 'North America',\n",
    "    'Costa Rica': 'North America',\n",
    "    'El Salvador': 'North America',\n",
    "    'Panama': 'North America',\n",
    "    'Jamaica': 'North America',\n",
    "    \n",
    "    # South America\n",
    "    'Colombia': 'South America',\n",
    "    'Brazil': 'South America',\n",
    "    'Peru': 'South America',\n",
    "    'Ecuador': 'South America',\n",
    "    'Bolivia': 'South America',\n",
    "    'Venezuela': 'South America',\n",
    "    \n",
    "    # Asia\n",
    "    'Taiwan': 'Asia',\n",
    "    'Thailand': 'Asia',\n",
    "    'Indonesia': 'Asia',\n",
    "    'Philippines': 'Asia',\n",
    "    'Vietnam': 'Asia',\n",
    "    'India': 'Asia',\n",
    "    'China': 'Asia',\n",
    "    'Japan': 'Asia',\n",
    "    'Myanmar': 'Asia',\n",
    "    'Laos': 'Asia',\n",
    "    'Yemen': 'Asia',\n",
    "    \n",
    "    # Africa\n",
    "    'Ethiopia': 'Africa',\n",
    "    'Kenya': 'Africa',\n",
    "    'Tanzania, United Republic Of': 'Africa',\n",
    "    'Uganda': 'Africa',\n",
    "    'Rwanda': 'Africa',\n",
    "    'Burundi': 'Africa',\n",
    "    'Malawi': 'Africa',\n",
    "    'Zambia': 'Africa',\n",
    "    'Zimbabwe': 'Africa',\n",
    "    'Madagascar': 'Africa',\n",
    "    'Cameroon': 'Africa',\n",
    "    'Ivory Coast': 'Africa',\n",
    "    'Ghana': 'Africa',\n",
    "    'Togo': 'Africa',\n",
    "    'Democratic Republic of the Congo': 'Africa',\n",
    "    \n",
    "    # Pacific/Oceania\n",
    "    'Papua New Guinea': 'Oceania',\n",
    "    'Hawaii': 'Oceania',  # Alternative name\n",
    "    \n",
    "    # Unknown/Missing\n",
    "    'Unknown': 'Unknown'\n",
    "}\n",
    "\n",
    "# Add continent column to dataframe\n",
    "df['continent'] = df['country_of_origin'].map(continent_mapping)\n",
    "\n",
    "# Check for any unmapped countries\n",
    "unmapped_countries = df[df['continent'].isna()]['country_of_origin'].unique()\n",
    "if len(unmapped_countries) > 0:\n",
    "    print(f\"⚠️ Warning: Unmapped countries found: {unmapped_countries}\")\n",
    "    print(\"Adding them to 'Unknown' continent category\")\n",
    "    df['continent'] = df['continent'].fillna('Unknown')\n",
    "\n",
    "print(f\"Continent mapping created!\")\n",
    "print(f\"Countries mapped to continents:\")\n",
    "continent_country_counts = df.groupby('continent')['country_of_origin'].nunique().sort_values(ascending=False)\n",
    "for continent, count in continent_country_counts.items():\n",
    "    print(f\"{continent}: {count} countries\")\n",
    "\n",
    "# Create continent vs color crosstab\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"CONTINENT vs COLOR TABLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "continent_color_table = pd.crosstab(\n",
    "    df['continent'], \n",
    "    df['color'], \n",
    "    margins=True,\n",
    "    margins_name='Total',\n",
    "    dropna=False  # Keep missing values as requested\n",
    ")\n",
    "\n",
    "print(\"Coffee color distribution by continent:\")\n",
    "print(\"(Continents in rows, colors in columns)\")\n",
    "print()\n",
    "print(continent_color_table.to_string())\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"CONTINENT COLOR ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate percentages for better understanding\n",
    "print(\"Color distribution percentages by continent:\")\n",
    "print(\"(Excluding Total row)\")\n",
    "\n",
    "continent_color_pct = pd.crosstab(\n",
    "    df['continent'], \n",
    "    df['color'], \n",
    "    normalize='index',  # Normalize by row (continent)\n",
    "    dropna=False\n",
    ") * 100\n",
    "\n",
    "# Round to 1 decimal place\n",
    "continent_color_pct = continent_color_pct.round(1)\n",
    "print(continent_color_pct.to_string())\n",
    "\n",
    "# Summary insights\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"1. COLOR DISTRIBUTION BY COUNTRY:\")\n",
    "total_records_with_color = df['color'].notna().sum()\n",
    "print(f\"   - {total_records_with_color} records have color information\")\n",
    "print(f\"   - {df['color'].isna().sum()} records have missing color data\")\n",
    "\n",
    "# Most common color overall\n",
    "most_common_color = df['color'].value_counts().index[0]\n",
    "most_common_count = df['color'].value_counts().iloc[0]\n",
    "print(f\"   - Most common color overall: {most_common_color} ({most_common_count} records)\")\n",
    "\n",
    "print(f\"\\n2. GEOGRAPHICAL DISTRIBUTION:\")\n",
    "print(f\"   - Total continents represented: {len(continent_color_table) - 1}\")\n",
    "print(f\"   - Total countries represented: {df['country_of_origin'].nunique()}\")\n",
    "\n",
    "# Which continent has most coffee records\n",
    "continent_totals = continent_color_table.loc[continent_color_table.index != 'Total', 'Total'].sort_values(ascending=False)\n",
    "print(f\"   - Continent with most coffee records: {continent_totals.index[0]} ({continent_totals.iloc[0]} records)\")\n",
    "\n",
    "print(f\"\\n3. COLOR DIVERSITY:\")\n",
    "print(f\"   - Most color-diverse continent:\")\n",
    "continent_diversity = (continent_color_table > 0).sum(axis=1)\n",
    "most_diverse_continent = continent_diversity[continent_diversity.index != 'Total'].sort_values(ascending=False).index[0]\n",
    "diversity_count = continent_diversity[most_diverse_continent]\n",
    "print(f\"     {most_diverse_continent} with {diversity_count} different colors\")\n",
    "\n",
    "# Show the detailed breakdown for the most diverse continent\n",
    "print(f\"\\n   - Color breakdown for {most_diverse_continent}:\")\n",
    "most_diverse_colors = continent_color_table.loc[most_diverse_continent]\n",
    "for color, count in most_diverse_colors[most_diverse_colors > 0].sort_values(ascending=False).items():\n",
    "    if color != 'Total':\n",
    "        percentage = (count / most_diverse_colors['Total']) * 100\n",
    "        print(f\"     {color}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"DATA EXPORT READY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"✓ Country vs Color table created (long format)\")\n",
    "print(\"✓ Continent vs Color table created (long format)\")  \n",
    "print(\"✓ Missing values preserved as requested\")\n",
    "print(\"✓ Both tables ready for further analysis\")\n",
    "\n",
    "# Optional: Save tables for external use\n",
    "print(f\"\\nTable dimensions:\")\n",
    "print(f\"Country-Color table: {country_color_table.shape}\")\n",
    "print(f\"Continent-Color table: {continent_color_table.shape}\")\n",
    "\n",
    "# Show first few rows of each table for verification\n",
    "print(f\"\\nFirst 5 countries in Country-Color table:\")\n",
    "print(country_color_table.head().to_string())\n",
    "\n",
    "print(f\"\\nComplete Continent-Color table:\")\n",
    "print(continent_color_table.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27095ced-7179-4ee4-94d3-7d94450b4120",
   "metadata": {},
   "source": [
    "### Problem 9. Ratings (1 point)\n",
    "The columns `Aroma`, `Flavor`, etc., up to `Moisture` represent subjective ratings. Explore them. Show the means and range; draw histograms and / or boxplots as needed. You can even try correlations if you want. What's up with all those ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ee355-dcbb-4657-a814-cdcfbd455c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92054218-978d-48c6-b7aa-36226837354c",
   "metadata": {},
   "source": [
    "### Problem 10. High-level errors (1 point)\n",
    "Check the countries against region names, altitudes, and companies. Are there any discrepancies (e.g. human errors, like a region not matching the country)? Take a look at the (cleaned) altitudes; there has been a lot of preprocessing done to them. Was it done correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971e5dd-4bb1-4ad6-bcbc-3cceab758f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c3b5118-9f8e-45c2-a200-1be89fa4b3bf",
   "metadata": {},
   "source": [
    "### * Problem 11. Clean and explore at will\n",
    "The dataset claimed to be clean, but we were able to discover a lot of things to fix and do better.\n",
    "\n",
    "Play around with the data as much as you wish, and if you find variables to tidy up and clean - by all means, do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928732cf-7ef4-471a-9818-139dd519eb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
